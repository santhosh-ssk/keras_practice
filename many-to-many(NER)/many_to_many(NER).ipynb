{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to find words frequency, maximum line length and number of lines in a given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def parse_sentences(filename):\n",
    "    words_freq=collections.Counter()\n",
    "    num_records,max_len=0,0\n",
    "    fin=open(filename,'r')\n",
    "    for line in fin:\n",
    "        words=line.strip().lower().split()\n",
    "        for word in words:\n",
    "            words_freq[word]+=1\n",
    "        if len(words)>max_len:\n",
    "            max_len=len(words)\n",
    "        num_records+=1\n",
    "    return words_freq,max_len,num_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute word frequency from treebank sentences and from treebank tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947 249 3914\n",
      "45 249 3914\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_dir=\"/home/santhosh/keras/many-to-many(NER)/data\"\n",
    "s_words_freq,s_max_len,s_num_records=parse_sentences(os.path.join(data_dir,\"treebank_sents.txt\"))\n",
    "t_words_freq,t_max_len,t_num_records=parse_sentences(os.path.join(data_dir,\"treebank_poss.txt\"))\n",
    "print(len(s_words_freq),s_max_len,s_num_records)\n",
    "print(len(t_words_freq),t_max_len,t_num_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from the above result, we can set max_line length=20 and set sentences vocabulary to 5000 and set tags vocabulary to 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen=20\n",
    "s_max_feature=5000\n",
    "t_max_feature=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vocabsize=min(len(s_words_freq),s_max_feature)+2\n",
    "t_vocabsize=t_max_feature+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate word2index  and index2word for 5000 most common words in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD',\n",
       " 1: 'UNK',\n",
       " 2: ',',\n",
       " 3: 'the',\n",
       " 4: '.',\n",
       " 5: 'of',\n",
       " 6: 'to',\n",
       " 7: 'a',\n",
       " 8: 'in',\n",
       " 9: 'and',\n",
       " 10: \"'s\",\n",
       " 11: 'for',\n",
       " 12: 'that',\n",
       " 13: '$',\n",
       " 14: '``',\n",
       " 15: \"''\",\n",
       " 16: 'is',\n",
       " 17: 'said',\n",
       " 18: 'it',\n",
       " 19: 'on',\n",
       " 20: '%',\n",
       " 21: 'by',\n",
       " 22: 'at',\n",
       " 23: 'as',\n",
       " 24: 'with',\n",
       " 25: 'from',\n",
       " 26: 'million',\n",
       " 27: 'mr.',\n",
       " 28: 'are',\n",
       " 29: 'was',\n",
       " 30: 'be',\n",
       " 31: 'its',\n",
       " 32: 'has',\n",
       " 33: 'an',\n",
       " 34: 'new',\n",
       " 35: 'have',\n",
       " 36: \"n't\",\n",
       " 37: 'but',\n",
       " 38: 'he',\n",
       " 39: 'or',\n",
       " 40: 'will',\n",
       " 41: 'they',\n",
       " 42: 'company',\n",
       " 43: '--',\n",
       " 44: 'which',\n",
       " 45: 'this',\n",
       " 46: 'u.s.',\n",
       " 47: 'says',\n",
       " 48: 'year',\n",
       " 49: 'about',\n",
       " 50: 'would',\n",
       " 51: 'more',\n",
       " 52: 'were',\n",
       " 53: 'market',\n",
       " 54: 'their',\n",
       " 55: 'than',\n",
       " 56: 'stock',\n",
       " 57: ';',\n",
       " 58: 'trading',\n",
       " 59: 'who',\n",
       " 60: 'had',\n",
       " 61: 'also',\n",
       " 62: 'president',\n",
       " 63: 'billion',\n",
       " 64: 'up',\n",
       " 65: 'one',\n",
       " 66: 'been',\n",
       " 67: 'some',\n",
       " 68: ':',\n",
       " 69: 'program',\n",
       " 70: 'other',\n",
       " 71: 'not',\n",
       " 72: 'his',\n",
       " 73: 'because',\n",
       " 74: 'if',\n",
       " 75: 'could',\n",
       " 76: 'share',\n",
       " 77: 'all',\n",
       " 78: 'corp.',\n",
       " 79: 'years',\n",
       " 80: 'i',\n",
       " 81: 'shares',\n",
       " 82: 'first',\n",
       " 83: '-rrb-',\n",
       " 84: 'two',\n",
       " 85: 'any',\n",
       " 86: 'york',\n",
       " 87: '-lrb-',\n",
       " 88: 'last',\n",
       " 89: 'many',\n",
       " 90: 'there',\n",
       " 91: 'she',\n",
       " 92: 'no',\n",
       " 93: 'such',\n",
       " 94: 'when',\n",
       " 95: 'inc.',\n",
       " 96: 'we',\n",
       " 97: 'can',\n",
       " 98: 'you',\n",
       " 99: 'so',\n",
       " 100: 'japanese',\n",
       " 101: 'prices',\n",
       " 102: 'do',\n",
       " 103: 'after',\n",
       " 104: 'government',\n",
       " 105: 'into',\n",
       " 106: 'business',\n",
       " 107: '&',\n",
       " 108: 'over',\n",
       " 109: 'only',\n",
       " 110: 'most',\n",
       " 111: 'may',\n",
       " 112: 'out',\n",
       " 113: 'sales',\n",
       " 114: 'these',\n",
       " 115: 'even',\n",
       " 116: 'federal',\n",
       " 117: 'say',\n",
       " 118: 'japan',\n",
       " 119: 'make',\n",
       " 120: 'under',\n",
       " 121: 'while',\n",
       " 122: 'co.',\n",
       " 123: \"'\",\n",
       " 124: 'board',\n",
       " 125: 'index',\n",
       " 126: 'exchange',\n",
       " 127: 'recent',\n",
       " 128: 'big',\n",
       " 129: 'price',\n",
       " 130: 'futures',\n",
       " 131: 'department',\n",
       " 132: 'time',\n",
       " 133: 'what',\n",
       " 134: 'them',\n",
       " 135: 'now',\n",
       " 136: 'cents',\n",
       " 137: 'bank',\n",
       " 138: 'stocks',\n",
       " 139: 'investors',\n",
       " 140: 'group',\n",
       " 141: 'next',\n",
       " 142: 'executive',\n",
       " 143: 'funds',\n",
       " 144: 'yesterday',\n",
       " 145: 'american',\n",
       " 146: 'companies',\n",
       " 147: 'investment',\n",
       " 148: 'profit',\n",
       " 149: 'trade',\n",
       " 150: 'much',\n",
       " 151: 'house',\n",
       " 152: 'like',\n",
       " 153: 'did',\n",
       " 154: 'people',\n",
       " 155: 'october',\n",
       " 156: 'those',\n",
       " 157: 'bonds',\n",
       " 158: 'rose',\n",
       " 159: 'money',\n",
       " 160: 'issue',\n",
       " 161: 'securities',\n",
       " 162: 'common',\n",
       " 163: 'months',\n",
       " 164: 'net',\n",
       " 165: 'financial',\n",
       " 166: 'mrs.',\n",
       " 167: 'markets',\n",
       " 168: 'chairman',\n",
       " 169: 'down',\n",
       " 170: 'buy',\n",
       " 171: 'made',\n",
       " 172: 'week',\n",
       " 173: 'since',\n",
       " 174: 'yen',\n",
       " 175: 'her',\n",
       " 176: 'research',\n",
       " 177: 'banks',\n",
       " 178: 'does',\n",
       " 179: 'take',\n",
       " 180: 'interest',\n",
       " 181: 'rates',\n",
       " 182: 'earlier',\n",
       " 183: 'expected',\n",
       " 184: 'higher',\n",
       " 185: 'high',\n",
       " 186: 'three',\n",
       " 187: 'days',\n",
       " 188: 'international',\n",
       " 189: 'chief',\n",
       " 190: '10',\n",
       " 191: 'plan',\n",
       " 192: 'officials',\n",
       " 193: 'before',\n",
       " 194: 'traders',\n",
       " 195: 'own',\n",
       " 196: 'yield',\n",
       " 197: 'report',\n",
       " 198: 'just',\n",
       " 199: 'another',\n",
       " 200: 'country',\n",
       " 201: 'offer',\n",
       " 202: 'court',\n",
       " 203: 'get',\n",
       " 204: 'debt',\n",
       " 205: '30',\n",
       " 206: 'congress',\n",
       " 207: 'each',\n",
       " 208: 'small',\n",
       " 209: '50',\n",
       " 210: 'sell',\n",
       " 211: 'well',\n",
       " 212: 'major',\n",
       " 213: 'tuesday',\n",
       " 214: 'economic',\n",
       " 215: 'state',\n",
       " 216: 'off',\n",
       " 217: '1988',\n",
       " 218: 'rate',\n",
       " 219: 'number',\n",
       " 220: 'industry',\n",
       " 221: 'vice',\n",
       " 222: 'among',\n",
       " 223: 'might',\n",
       " 224: 'both',\n",
       " 225: 'bush',\n",
       " 226: 'month',\n",
       " 227: 'still',\n",
       " 228: 'chicago',\n",
       " 229: 'cash',\n",
       " 230: 'early',\n",
       " 231: '1990',\n",
       " 232: 'during',\n",
       " 233: 'help',\n",
       " 234: '1',\n",
       " 235: 'growth',\n",
       " 236: 'income',\n",
       " 237: 'test',\n",
       " 238: 'average',\n",
       " 239: 'street',\n",
       " 240: 'good',\n",
       " 241: '15',\n",
       " 242: 'treasury',\n",
       " 243: 'capital',\n",
       " 244: 'fiscal',\n",
       " 245: 'concern',\n",
       " 246: 'september',\n",
       " 247: 'according',\n",
       " 248: 'through',\n",
       " 249: 'south',\n",
       " 250: 'law',\n",
       " 251: 'pay',\n",
       " 252: 'due',\n",
       " 253: 'investor',\n",
       " 254: 'less',\n",
       " 255: 'earnings',\n",
       " 256: '1989',\n",
       " 257: 'issues',\n",
       " 258: 'added',\n",
       " 259: '?',\n",
       " 260: 'managers',\n",
       " 261: '100',\n",
       " 262: 'firm',\n",
       " 263: 'part',\n",
       " 264: 'sold',\n",
       " 265: 'spokesman',\n",
       " 266: 'analysts',\n",
       " 267: 'including',\n",
       " 268: 'use',\n",
       " 269: 'several',\n",
       " 270: 'ago',\n",
       " 271: 'should',\n",
       " 272: '500',\n",
       " 273: 'school',\n",
       " 274: 'columbia',\n",
       " 275: 'yeargin',\n",
       " 276: 'same',\n",
       " 277: 'contract',\n",
       " 278: 'reported',\n",
       " 279: 'wall',\n",
       " 280: 'world',\n",
       " 281: 'already',\n",
       " 282: 'right',\n",
       " 283: '8',\n",
       " 284: 'computer',\n",
       " 285: 'until',\n",
       " 286: 'bill',\n",
       " 287: 'plans',\n",
       " 288: 'cray',\n",
       " 289: 'quarter',\n",
       " 290: 'between',\n",
       " 291: 'unit',\n",
       " 292: 'where',\n",
       " 293: 'bid',\n",
       " 294: 'few',\n",
       " 295: 'firms',\n",
       " 296: 'against',\n",
       " 297: 'without',\n",
       " 298: 's&p',\n",
       " 299: 'however',\n",
       " 300: 'administration',\n",
       " 301: 'general',\n",
       " 302: 'john',\n",
       " 303: 'management',\n",
       " 304: 'service',\n",
       " 305: 'products',\n",
       " 306: 'public',\n",
       " 307: 'current',\n",
       " 308: 'being',\n",
       " 309: 'then',\n",
       " 310: 'far',\n",
       " 311: 'used',\n",
       " 312: 'below',\n",
       " 313: 'compared',\n",
       " 314: 'city',\n",
       " 315: 'notes',\n",
       " 316: 'paper',\n",
       " 317: 'old',\n",
       " 318: 'director',\n",
       " 319: 'fell',\n",
       " 320: '1987',\n",
       " 321: 'officer',\n",
       " 322: 'began',\n",
       " 323: 'loss',\n",
       " 324: 'based',\n",
       " 325: 'construction',\n",
       " 326: 'very',\n",
       " 327: 'corporate',\n",
       " 328: 'operations',\n",
       " 329: 'students',\n",
       " 330: 'health',\n",
       " 331: 'points',\n",
       " 332: 'lower',\n",
       " 333: 'six',\n",
       " 334: 'association',\n",
       " 335: 'national',\n",
       " 336: 'increase',\n",
       " 337: 'end',\n",
       " 338: 'charge',\n",
       " 339: 'demand',\n",
       " 340: 'news',\n",
       " 341: 'orders',\n",
       " 342: 'though',\n",
       " 343: 'record',\n",
       " 344: 'around',\n",
       " 345: 'services',\n",
       " 346: 'your',\n",
       " 347: 'closed',\n",
       " 348: 'junk',\n",
       " 349: 'five',\n",
       " 350: 'way',\n",
       " 351: 'problem',\n",
       " 352: '2',\n",
       " 353: 'information',\n",
       " 354: 'see',\n",
       " 355: 'sale',\n",
       " 356: 'past',\n",
       " 357: 'whether',\n",
       " 358: 'bond',\n",
       " 359: \"'re\",\n",
       " 360: 'ad',\n",
       " 361: 'point',\n",
       " 362: 'university',\n",
       " 363: 'large',\n",
       " 364: 'want',\n",
       " 365: 'set',\n",
       " 366: 'case',\n",
       " 367: 'foreign',\n",
       " 368: 'today',\n",
       " 369: 'close',\n",
       " 370: 'took',\n",
       " 371: 'return',\n",
       " 372: 'how',\n",
       " 373: 'largest',\n",
       " 374: 'us',\n",
       " 375: 'long',\n",
       " 376: 'little',\n",
       " 377: 'cut',\n",
       " 378: 'problems',\n",
       " 379: 'maker',\n",
       " 380: 'london',\n",
       " 381: '3\\\\/4',\n",
       " 382: 'safety',\n",
       " 383: 'although',\n",
       " 384: 'analyst',\n",
       " 385: 'offered',\n",
       " 386: 'appropriations',\n",
       " 387: 'work',\n",
       " 388: 'spending',\n",
       " 389: 'manager',\n",
       " 390: 'economy',\n",
       " 391: 'finance',\n",
       " 392: 'currently',\n",
       " 393: 'action',\n",
       " 394: 'example',\n",
       " 395: 'campbell',\n",
       " 396: '1\\\\/2',\n",
       " 397: 'terms',\n",
       " 398: 'purchase',\n",
       " 399: 'our',\n",
       " 400: 'offering',\n",
       " 401: 'union',\n",
       " 402: 'back',\n",
       " 403: 'suspension',\n",
       " 404: 'future',\n",
       " 405: 'contracts',\n",
       " 406: '13',\n",
       " 407: 'additional',\n",
       " 408: 'members',\n",
       " 409: 'costs',\n",
       " 410: 'priced',\n",
       " 411: 'commercial',\n",
       " 412: 'california',\n",
       " 413: 'force',\n",
       " 414: 'certain',\n",
       " 415: 'materials',\n",
       " 416: 'total',\n",
       " 417: 'move',\n",
       " 418: 'power',\n",
       " 419: 'operating',\n",
       " 420: 'recently',\n",
       " 421: 'despite',\n",
       " 422: 'nation',\n",
       " 423: 'strong',\n",
       " 424: 'low',\n",
       " 425: 'volume',\n",
       " 426: 'division',\n",
       " 427: 'put',\n",
       " 428: 'fund',\n",
       " 429: 'industrial',\n",
       " 430: 'ended',\n",
       " 431: 'noted',\n",
       " 432: 'period',\n",
       " 433: 'fall',\n",
       " 434: 'come',\n",
       " 435: '20',\n",
       " 436: 'possible',\n",
       " 437: 'value',\n",
       " 438: 'china',\n",
       " 439: 'think',\n",
       " 440: 'oct.',\n",
       " 441: 'fine',\n",
       " 442: 'often',\n",
       " 443: 'committee',\n",
       " 444: 'approved',\n",
       " 445: 'savings',\n",
       " 446: 'customers',\n",
       " 447: 'insurance',\n",
       " 448: 'fees',\n",
       " 449: 'family',\n",
       " 450: 'arbitrage',\n",
       " 451: 'least',\n",
       " 452: 'programs',\n",
       " 453: 'dividend',\n",
       " 454: 'commission',\n",
       " 455: 'ringers',\n",
       " 456: 'local',\n",
       " 457: 'stake',\n",
       " 458: 'credit',\n",
       " 459: 'dealers',\n",
       " 460: 'office',\n",
       " 461: 'manufacturing',\n",
       " 462: 'robert',\n",
       " 463: 'nov.',\n",
       " 464: '40',\n",
       " 465: 'tokyo',\n",
       " 466: 'day',\n",
       " 467: 'result',\n",
       " 468: 'too',\n",
       " 469: 'sugar',\n",
       " 470: 'loan',\n",
       " 471: 'card',\n",
       " 472: 'washington',\n",
       " 473: 'face',\n",
       " 474: 'usx',\n",
       " 475: 'growing',\n",
       " 476: 'development',\n",
       " 477: 'judge',\n",
       " 478: 'takeover',\n",
       " 479: 'legislation',\n",
       " 480: 'almost',\n",
       " 481: 'georgia',\n",
       " 482: 'become',\n",
       " 483: 'holding',\n",
       " 484: 'increased',\n",
       " 485: 'yet',\n",
       " 486: 'addition',\n",
       " 487: '12',\n",
       " 488: 'named',\n",
       " 489: 'voice',\n",
       " 490: 'volatility',\n",
       " 491: 'half',\n",
       " 492: 'england',\n",
       " 493: 'magazine',\n",
       " 494: 'here',\n",
       " 495: 'oil',\n",
       " 496: 'found',\n",
       " 497: 'william',\n",
       " 498: 'veto',\n",
       " 499: 'countries',\n",
       " 500: 'cases',\n",
       " 501: 'latest',\n",
       " 502: 'ii',\n",
       " 503: 'go',\n",
       " 504: 'estimated',\n",
       " 505: 'private',\n",
       " 506: 'gains',\n",
       " 507: 'results',\n",
       " 508: 'james',\n",
       " 509: 'workers',\n",
       " 510: 'payments',\n",
       " 511: 'support',\n",
       " 512: 'questions',\n",
       " 513: 'declined',\n",
       " 514: 'received',\n",
       " 515: 'weeks',\n",
       " 516: 'different',\n",
       " 517: 'banking',\n",
       " 518: 'proposed',\n",
       " 519: 'march',\n",
       " 520: 'production',\n",
       " 521: 'great',\n",
       " 522: 'likely',\n",
       " 523: 'level',\n",
       " 524: 'acquisition',\n",
       " 525: 'change',\n",
       " 526: 'agency',\n",
       " 527: 'system',\n",
       " 528: 'stock-index',\n",
       " 529: 'trying',\n",
       " 530: 'full',\n",
       " 531: 'four',\n",
       " 532: 'edison',\n",
       " 533: 'outstanding',\n",
       " 534: 'give',\n",
       " 535: 'dividends',\n",
       " 536: 'product',\n",
       " 537: 'proposal',\n",
       " 538: 'hahn',\n",
       " 539: 'corn',\n",
       " 540: 'trades',\n",
       " 541: 'senior',\n",
       " 542: 'raise',\n",
       " 543: 'plant',\n",
       " 544: 'transactions',\n",
       " 545: 'never',\n",
       " 546: 'rise',\n",
       " 547: 'decline',\n",
       " 548: 'every',\n",
       " 549: 'buying',\n",
       " 550: 'held',\n",
       " 551: 'home',\n",
       " 552: '15,000',\n",
       " 553: 'wine',\n",
       " 554: 'act',\n",
       " 555: 'funding',\n",
       " 556: 'america',\n",
       " 557: 'came',\n",
       " 558: 'limit',\n",
       " 559: 'white',\n",
       " 560: 'shareholders',\n",
       " 561: 'times',\n",
       " 562: 'talks',\n",
       " 563: 'making',\n",
       " 564: 'dow',\n",
       " 565: 'continue',\n",
       " 566: 'place',\n",
       " 567: 'lead',\n",
       " 568: 'political',\n",
       " 569: 'bells',\n",
       " 570: 'bills',\n",
       " 571: '3',\n",
       " 572: 'rep.',\n",
       " 573: '7',\n",
       " 574: 'paid',\n",
       " 575: 'gulf',\n",
       " 576: 'official',\n",
       " 577: 'called',\n",
       " 578: 'late',\n",
       " 579: 'designed',\n",
       " 580: 'my',\n",
       " 581: 'employees',\n",
       " 582: 'jones',\n",
       " 583: 'transaction',\n",
       " 584: 'rights',\n",
       " 585: 'going',\n",
       " 586: 'deal',\n",
       " 587: 'seeking',\n",
       " 588: 'comment',\n",
       " 589: 'express',\n",
       " 590: '5',\n",
       " 591: 'better',\n",
       " 592: 'inc',\n",
       " 593: 'food',\n",
       " 594: 'san',\n",
       " 595: 'enough',\n",
       " 596: 'order',\n",
       " 597: 'previous',\n",
       " 598: 'commerce',\n",
       " 599: 'cars',\n",
       " 600: 'restructuring',\n",
       " 601: 'assets',\n",
       " 602: 'ltd.',\n",
       " 603: 'pressure',\n",
       " 604: 'life',\n",
       " 605: 'decision',\n",
       " 606: 'expects',\n",
       " 607: 'gain',\n",
       " 608: 'industries',\n",
       " 609: 'senate',\n",
       " 610: 'korea',\n",
       " 611: 'wo',\n",
       " 612: 'soviet',\n",
       " 613: 'further',\n",
       " 614: 'united',\n",
       " 615: 'must',\n",
       " 616: 'former',\n",
       " 617: 'amount',\n",
       " 618: 'above',\n",
       " 619: 'executives',\n",
       " 620: 'drop',\n",
       " 621: 'institutions',\n",
       " 622: 'clients',\n",
       " 623: 'got',\n",
       " 624: 'second',\n",
       " 625: 'gained',\n",
       " 626: 'investments',\n",
       " 627: '18',\n",
       " 628: 'performance',\n",
       " 629: 'publishing',\n",
       " 630: 'dollar',\n",
       " 631: 'fined',\n",
       " 632: 'august',\n",
       " 633: 'reached',\n",
       " 634: 'increasing',\n",
       " 635: '1991',\n",
       " 636: 'equity',\n",
       " 637: 'nixon',\n",
       " 638: 'form',\n",
       " 639: 'thrift',\n",
       " 640: 'conditions',\n",
       " 641: 'a.',\n",
       " 642: 'drug',\n",
       " 643: 'saying',\n",
       " 644: 'states',\n",
       " 645: 'acquired',\n",
       " 646: 'buick',\n",
       " 647: 'job',\n",
       " 648: 'particularly',\n",
       " 649: 'standard',\n",
       " 650: 'need',\n",
       " 651: 'purchasing',\n",
       " 652: 'run',\n",
       " 653: '10,000',\n",
       " 654: 'free',\n",
       " 655: 'survey',\n",
       " 656: '9',\n",
       " 657: 'rather',\n",
       " 658: 'wage',\n",
       " 659: 'owns',\n",
       " 660: 'composite',\n",
       " 661: 'control',\n",
       " 662: 'team',\n",
       " 663: 'farmers',\n",
       " 664: 'once',\n",
       " 665: 'real',\n",
       " 666: 'effect',\n",
       " 667: 'young',\n",
       " 668: 'institute',\n",
       " 669: 'dec.',\n",
       " 670: 'agreement',\n",
       " 671: 'activity',\n",
       " 672: 'dollars',\n",
       " 673: 'nearly',\n",
       " 674: 'containers',\n",
       " 675: 'away',\n",
       " 676: 'tax',\n",
       " 677: 'light',\n",
       " 678: 'nations',\n",
       " 679: 'package',\n",
       " 680: '1985',\n",
       " 681: 'became',\n",
       " 682: '11',\n",
       " 683: 'included',\n",
       " 684: 'agreed',\n",
       " 685: 'charges',\n",
       " 686: 'michael',\n",
       " 687: 'seek',\n",
       " 688: 'following',\n",
       " 689: '6',\n",
       " 690: 'marketing',\n",
       " 691: 'filed',\n",
       " 692: 'important',\n",
       " 693: 'steel',\n",
       " 694: 'announced',\n",
       " 695: 'texas',\n",
       " 696: 'annual',\n",
       " 697: 'campaign',\n",
       " 698: 'boston',\n",
       " 699: 'sector',\n",
       " 700: 'keep',\n",
       " 701: 'always',\n",
       " 702: 'using',\n",
       " 703: 'failed',\n",
       " 704: 'ford',\n",
       " 705: 'disclosed',\n",
       " 706: 'ual',\n",
       " 707: 'region',\n",
       " 708: 'article',\n",
       " 709: 'r.',\n",
       " 710: 'coming',\n",
       " 711: 'know',\n",
       " 712: 'posted',\n",
       " 713: 'moody',\n",
       " 714: 'violations',\n",
       " 715: 'budget',\n",
       " 716: 'him',\n",
       " 717: 'tests',\n",
       " 718: 'cost',\n",
       " 719: 'able',\n",
       " 720: 'women',\n",
       " 721: '4',\n",
       " 722: 'poor',\n",
       " 723: 'taken',\n",
       " 724: 'factory',\n",
       " 725: 'delivery',\n",
       " 726: 'lawyers',\n",
       " 727: 'car',\n",
       " 728: 'co',\n",
       " 729: 'policy',\n",
       " 730: 'data',\n",
       " 731: 'calif.',\n",
       " 732: 'section',\n",
       " 733: 'changes',\n",
       " 734: 'meeting',\n",
       " 735: 'traded',\n",
       " 736: 'shearson',\n",
       " 737: 'principal',\n",
       " 738: '31',\n",
       " 739: 'grain',\n",
       " 740: '...',\n",
       " 741: 'third',\n",
       " 742: 'led',\n",
       " 743: 'energy',\n",
       " 744: '#',\n",
       " 745: 'telephone',\n",
       " 746: 'computers',\n",
       " 747: 'sea',\n",
       " 748: \"'ve\",\n",
       " 749: 'decided',\n",
       " 750: 'find',\n",
       " 751: 'increases',\n",
       " 752: 'wednesday',\n",
       " 753: 'clear',\n",
       " 754: 'church',\n",
       " 755: 'consumer',\n",
       " 756: 'jobs',\n",
       " 757: 'built',\n",
       " 758: 'speculation',\n",
       " 759: 'personal',\n",
       " 760: 'completed',\n",
       " 761: 'things',\n",
       " 762: '17',\n",
       " 763: 'asia',\n",
       " 764: 'needed',\n",
       " 765: 'defense',\n",
       " 766: 'st.',\n",
       " 767: 'continued',\n",
       " 768: 'mitsubishi',\n",
       " 769: 'ward',\n",
       " 770: 'via',\n",
       " 771: 'raised',\n",
       " 772: 'justice',\n",
       " 773: 'potential',\n",
       " 774: 'imports',\n",
       " 775: 'hills',\n",
       " 776: 'trucks',\n",
       " 777: 'lost',\n",
       " 778: 'buy-out',\n",
       " 779: 'across',\n",
       " 780: 'statement',\n",
       " 781: 'review',\n",
       " 782: 'long-term',\n",
       " 783: 'nl',\n",
       " 784: 'fact',\n",
       " 785: 'j.',\n",
       " 786: 'probably',\n",
       " 787: 'reports',\n",
       " 788: 'series',\n",
       " 789: 'd.',\n",
       " 790: 'huge',\n",
       " 791: 'closely',\n",
       " 792: 'wines',\n",
       " 793: 'special',\n",
       " 794: 'again',\n",
       " 795: 'phelan',\n",
       " 796: '3\\\\/8',\n",
       " 797: 'rule',\n",
       " 798: 'provide',\n",
       " 799: 'care',\n",
       " 800: 'river',\n",
       " 801: '-rcb-',\n",
       " 802: 'line',\n",
       " 803: 'teacher',\n",
       " 804: 'revenue',\n",
       " 805: 'building',\n",
       " 806: 'changed',\n",
       " 807: 'include',\n",
       " 808: 'scheduled',\n",
       " 809: 'told',\n",
       " 810: 'june',\n",
       " 811: 'bad',\n",
       " 812: 'whose',\n",
       " 813: 'options',\n",
       " 814: 'wants',\n",
       " 815: 'indicated',\n",
       " 816: 'previously',\n",
       " 817: 'herald',\n",
       " 818: 'december',\n",
       " 819: 'minimum',\n",
       " 820: 'helped',\n",
       " 821: 'district',\n",
       " 822: 'planned',\n",
       " 823: 'went',\n",
       " 824: 'capacity',\n",
       " 825: 'cause',\n",
       " 826: 'thing',\n",
       " 827: 'others',\n",
       " 828: 'why',\n",
       " 829: 'rating',\n",
       " 830: 'question',\n",
       " 831: 'johnson',\n",
       " 832: 'individual',\n",
       " 833: 'u.k.',\n",
       " 834: 'holders',\n",
       " 835: 'francisco',\n",
       " 836: 'look',\n",
       " 837: 'software',\n",
       " 838: 'fed',\n",
       " 839: 'kind',\n",
       " 840: 'show',\n",
       " 841: 'parent',\n",
       " 842: 'aid',\n",
       " 843: 'meanwhile',\n",
       " 844: 'continuing',\n",
       " 845: 'ms.',\n",
       " 846: 'bell',\n",
       " 847: 'receive',\n",
       " 848: 'short',\n",
       " 849: 'antitrust',\n",
       " 850: 'conference',\n",
       " 851: 'regulators',\n",
       " 852: '-lcb-',\n",
       " 853: 'significant',\n",
       " 854: 'abortion',\n",
       " 855: 'generally',\n",
       " 856: 'something',\n",
       " 857: 'later',\n",
       " 858: 'believe',\n",
       " 859: 'account',\n",
       " 860: 'short-term',\n",
       " 861: 'paying',\n",
       " 862: 'financing',\n",
       " 863: 'domestic',\n",
       " 864: 'matter',\n",
       " 865: 'working',\n",
       " 866: 'man',\n",
       " 867: 'thomas',\n",
       " 868: 'researchers',\n",
       " 869: 'goes',\n",
       " 870: 'evidence',\n",
       " 871: 'commonwealth',\n",
       " 872: 'pretax',\n",
       " 873: 'david',\n",
       " 874: 'irs',\n",
       " 875: 'apparently',\n",
       " 876: 'available',\n",
       " 877: '70',\n",
       " 878: 'ruling',\n",
       " 879: 'export',\n",
       " 880: 'concerns',\n",
       " 881: 'dr.',\n",
       " 882: 'auto',\n",
       " 883: '90',\n",
       " 884: '5\\\\/8',\n",
       " 885: 'partners',\n",
       " 886: 'west',\n",
       " 887: 'study',\n",
       " 888: 'nine',\n",
       " 889: 'leading',\n",
       " 890: 'levels',\n",
       " 891: 'traditional',\n",
       " 892: 'gives',\n",
       " 893: 'drexel',\n",
       " 894: 'within',\n",
       " 895: 'authority',\n",
       " 896: 'sept.',\n",
       " 897: 'newspaper',\n",
       " 898: 'sony',\n",
       " 899: 'stores',\n",
       " 900: 'fujitsu',\n",
       " 901: 'plc',\n",
       " 902: 'advertising',\n",
       " 903: 'wanted',\n",
       " 904: 'greenville',\n",
       " 905: 'scoring',\n",
       " 906: 'richard',\n",
       " 907: 'advertisers',\n",
       " 908: 'transportation',\n",
       " 909: 'departure',\n",
       " 910: 'airline',\n",
       " 911: '60',\n",
       " 912: 'strike',\n",
       " 913: 'homeless',\n",
       " 914: 'legal',\n",
       " 915: \"'m\",\n",
       " 916: 'pence',\n",
       " 917: 'reduce',\n",
       " 918: 'carolina',\n",
       " 919: 'preferred',\n",
       " 920: 'remains',\n",
       " 921: 'losses',\n",
       " 922: 'dropped',\n",
       " 923: 'situation',\n",
       " 924: 'position',\n",
       " 925: 'los',\n",
       " 926: '25',\n",
       " 927: 'anything',\n",
       " 928: 'advanced',\n",
       " 929: 'process',\n",
       " 930: 'mark',\n",
       " 931: 'profits',\n",
       " 932: 'third-quarter',\n",
       " 933: 'slow',\n",
       " 934: 'fields',\n",
       " 935: 'caused',\n",
       " 936: 'instead',\n",
       " 937: 'ordered',\n",
       " 938: 'showed',\n",
       " 939: 'known',\n",
       " 940: 'key',\n",
       " 941: 'convertible',\n",
       " 942: 'hearst',\n",
       " 943: 'buyers',\n",
       " 944: 'launched',\n",
       " 945: 'soon',\n",
       " 946: 'alleged',\n",
       " 947: 'wedtech',\n",
       " 948: 'person',\n",
       " 949: 'goods',\n",
       " 950: 'western',\n",
       " 951: '7\\\\/8',\n",
       " 952: 'crash',\n",
       " 953: 'organization',\n",
       " 954: 'continues',\n",
       " 955: 'daily',\n",
       " 956: 'april',\n",
       " 957: 'quickly',\n",
       " 958: 'waertsilae',\n",
       " 959: 'figures',\n",
       " 960: 'ban',\n",
       " 961: '16',\n",
       " 962: 'believes',\n",
       " 963: 'improve',\n",
       " 964: 'shareholder',\n",
       " 965: 'usia',\n",
       " 966: 'really',\n",
       " 967: 'random',\n",
       " 968: 'top',\n",
       " 969: 'adds',\n",
       " 970: 'sign',\n",
       " 971: 'jr.',\n",
       " 972: 'having',\n",
       " 973: 'mcgovern',\n",
       " 974: 'exports',\n",
       " 975: '14',\n",
       " 976: 'neither',\n",
       " 977: 'overseas',\n",
       " 978: 'remain',\n",
       " 979: 'ability',\n",
       " 980: 'units',\n",
       " 981: 'involved',\n",
       " 982: 'highly',\n",
       " 983: 'nor',\n",
       " 984: 'history',\n",
       " 985: 'des',\n",
       " 986: 'view',\n",
       " 987: 'partly',\n",
       " 988: 'given',\n",
       " 989: 'head',\n",
       " 990: 'slowing',\n",
       " 991: 'largely',\n",
       " 992: 'risk',\n",
       " 993: '1\\\\/4',\n",
       " 994: 'fully',\n",
       " 995: 'themselves',\n",
       " 996: 'perhaps',\n",
       " 997: 'returns',\n",
       " 998: 'valley',\n",
       " 999: 'ca',\n",
       " ...}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_word2index={x[0]:i+2 for i,x in enumerate(s_words_freq.most_common(s_max_feature))}\n",
    "s_word2index['PAD']=0\n",
    "s_word2index['UNK']=1\n",
    "s_index2word={v:k for k,v  in s_word2index.items()}\n",
    "s_index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate word2index  and index2word for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD',\n",
       " 1: 'nn',\n",
       " 2: 'in',\n",
       " 3: 'nnp',\n",
       " 4: 'dt',\n",
       " 5: 'nns',\n",
       " 6: 'jj',\n",
       " 7: ',',\n",
       " 8: '.',\n",
       " 9: 'cd',\n",
       " 10: 'vbd',\n",
       " 11: 'rb',\n",
       " 12: 'vb',\n",
       " 13: 'cc',\n",
       " 14: 'to',\n",
       " 15: 'vbn',\n",
       " 16: 'vbz',\n",
       " 17: 'prp',\n",
       " 18: 'vbg',\n",
       " 19: 'vbp',\n",
       " 20: 'md',\n",
       " 21: 'pos',\n",
       " 22: 'prp$',\n",
       " 23: '$',\n",
       " 24: '``',\n",
       " 25: \"''\",\n",
       " 26: ':',\n",
       " 27: 'wdt',\n",
       " 28: 'jjr',\n",
       " 29: 'nnps',\n",
       " 30: 'wp',\n",
       " 31: 'rp',\n",
       " 32: 'jjs',\n",
       " 33: 'wrb',\n",
       " 34: 'rbr',\n",
       " 35: '-rrb-',\n",
       " 36: '-lrb-',\n",
       " 37: 'ex',\n",
       " 38: 'rbs',\n",
       " 39: 'pdt',\n",
       " 40: '#',\n",
       " 41: 'wp$',\n",
       " 42: 'ls',\n",
       " 43: 'fw',\n",
       " 44: 'uh',\n",
       " 45: 'sym'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_word2index={x[0]:i+1 for i,x in enumerate(t_words_freq.most_common(t_max_feature))}\n",
    "t_word2index['PAD']=0\n",
    "t_index2word={v:k for k,v in t_word2index.items()}\n",
    "t_index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now create a dataset to train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X is going to contain rows of sentences and each row contains sequence of word index\n",
    "### Y is going to contain rows of sentences and each row contains sequence of tags index in one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "def  build_tensor(filename,numrecords,word2index,maxlen,make_category=False,num_classes=0):\n",
    "    data=np.empty((numrecords),dtype=list)\n",
    "    fin=open(filename,'r')\n",
    "    i=0\n",
    "    for line in fin:\n",
    "        words=line.strip().lower().split()\n",
    "        wids=[]\n",
    "        for word in words:\n",
    "            if word in word2index:\n",
    "                wids.append(word2index[word])\n",
    "            else:\n",
    "                wids.append(word2index['UNK'])\n",
    "        #print(wids)\n",
    "        if make_category:\n",
    "            data[i]=np_utils.to_categorical(wids,num_classes=num_classes)\n",
    "        else:\n",
    "            data[i]=wids\n",
    "        i+=1\n",
    "    fin.close()\n",
    "    pdata=sequence.pad_sequences(data,maxlen=maxlen)\n",
    "    return pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = build_tensor(os.path.join(data_dir,\"treebank_sents.txt\"),s_num_records,s_word2index,max_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y =build_tensor(os.path.join(data_dir,\"treebank_poss.txt\"),t_num_records,t_word2index,max_seqlen,True,t_vocabsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0,    0,    1, 4844,    2, 2148,   79,  317,    2,   40, 2703,\n",
       "           3,  124,   23,    7, 2304,  318,  463, 2111,    4], dtype=int32),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]], dtype=int32))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],Y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing test train module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation, Dense, Dropout, RepeatVector,SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 46\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5\n",
    "print(max_seqlen,t_vocabsize)\n",
    "NUM_OF_ITERATION=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating the model and then testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 14s 5ms/step - loss: 2.7007 - acc: 0.1211 - val_loss: 2.5919 - val_acc: 0.1342\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.4886 - acc: 0.1484 - val_loss: 2.4722 - val_acc: 0.1784\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.3989 - acc: 0.1688 - val_loss: 2.4199 - val_acc: 0.1811\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.3258 - acc: 0.1870 - val_loss: 2.3874 - val_acc: 0.1785\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.2814 - acc: 0.1954 - val_loss: 2.3239 - val_acc: 0.1891\n",
      "783/783 [==============================] - 1s 935us/step\n",
      "Test score: 2.324, accuracy: 0.189\n",
      "['$', '240', 'million', 'in', 'credit', 'and', 'loan', 'guarantees', 'in', 'fiscal', '1990', 'in', 'hopes', 'of', 'UNK', 'future', 'trade', 'and', 'investment', '.'] \n",
      "\n",
      " ['nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'cd', 'cd', '.', '.']\n",
      "Iteration:  2\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.2660 - acc: 0.1996 - val_loss: 2.3505 - val_acc: 0.1787\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.2269 - acc: 0.2119 - val_loss: 2.2833 - val_acc: 0.2054\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.2106 - acc: 0.2154 - val_loss: 2.2587 - val_acc: 0.2167\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.1723 - acc: 0.2250 - val_loss: 2.2574 - val_acc: 0.2072\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.1686 - acc: 0.2227 - val_loss: 2.2344 - val_acc: 0.2133\n",
      "783/783 [==============================] - 1s 896us/step\n",
      "Test score: 2.234, accuracy: 0.213\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'continued', 'export', 'demand', 'also', 'supported', 'prices', '.'] \n",
      "\n",
      " ['nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', '.']\n",
      "Iteration:  3\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.1234 - acc: 0.2328 - val_loss: 2.1946 - val_acc: 0.2264\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.0981 - acc: 0.2390 - val_loss: 2.1689 - val_acc: 0.2291\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.0613 - acc: 0.2460 - val_loss: 2.1391 - val_acc: 0.2345\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.0373 - acc: 0.2486 - val_loss: 2.1358 - val_acc: 0.2354\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 2.0355 - acc: 0.2483 - val_loss: 2.1797 - val_acc: 0.2174\n",
      "783/783 [==============================] - 1s 924us/step\n",
      "Test score: 2.180, accuracy: 0.217\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'he', 'declined', 'to', 'discuss', 'other', 'terms', 'of', 'the', 'issue', '.'] \n",
      "\n",
      " ['nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', ',', 'prp', 'prp', 'prp', 'vb', 'vb', 'vb', 'vb', 'vb', 'dt', 'dt', 'nn', 'nn', '.']\n",
      "Iteration:  4\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.9981 - acc: 0.2567 - val_loss: 2.0856 - val_acc: 0.2463\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.9585 - acc: 0.2668 - val_loss: 2.0608 - val_acc: 0.2496\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.9448 - acc: 0.2691 - val_loss: 2.0741 - val_acc: 0.2460\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.9307 - acc: 0.2712 - val_loss: 2.0305 - val_acc: 0.2568\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.8880 - acc: 0.2820 - val_loss: 2.0280 - val_acc: 0.2592\n",
      "783/783 [==============================] - 1s 882us/step\n",
      "Test score: 2.028, accuracy: 0.259\n",
      "['size', 'of', 'penalties', 'sought', 'by', 'osha', 'have', 'been', 'rising', 'in', 'recent', 'years', 'even', 'before', 'he', 'took', 'office', 'this', 'year', '.'] \n",
      "\n",
      " ['nn', 'in', 'in', 'in', 'nns', 'nns', 'nns', 'nns', 'nns', 'in', 'in', 'nns', 'nns', 'nns', 'rb', 'vbd', 'dt', 'dt', 'nn', '.']\n",
      "Iteration:  5\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.8679 - acc: 0.2844 - val_loss: 1.9902 - val_acc: 0.2677\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.8593 - acc: 0.2841 - val_loss: 2.0326 - val_acc: 0.2481\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.8244 - acc: 0.2931 - val_loss: 1.9628 - val_acc: 0.2718\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.8025 - acc: 0.2984 - val_loss: 1.9446 - val_acc: 0.2796\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.7909 - acc: 0.3003 - val_loss: 1.9320 - val_acc: 0.2800\n",
      "783/783 [==============================] - 1s 830us/step\n",
      "Test score: 1.932, accuracy: 0.280\n",
      "['demand', 'for', 'wines', 'from', 'the', 'UNK', 'and', 'UNK', ',', 'which', 'go', 'for', '$', '300', 'to', '$', '400', 'a', 'bottle', '.'] \n",
      "\n",
      " ['in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'cd', 'cd', 'cd', 'to', '$', '$', '$', 'in', 'dt', 'nn', '.']\n",
      "Iteration:  6\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.7580 - acc: 0.3087 - val_loss: 1.9130 - val_acc: 0.2828\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.7371 - acc: 0.3145 - val_loss: 1.9190 - val_acc: 0.2811\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.7237 - acc: 0.3165 - val_loss: 1.9053 - val_acc: 0.2884\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.7336 - acc: 0.3114 - val_loss: 1.8978 - val_acc: 0.2877\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.6859 - acc: 0.3250 - val_loss: 1.8701 - val_acc: 0.2960\n",
      "783/783 [==============================] - 1s 871us/step\n",
      "Test score: 1.870, accuracy: 0.296\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'evidence', 'of', 'widespread', 'cheating', 'has', 'surfaced', 'in', 'several', 'states', 'in', 'the', 'last', 'year', 'or', 'so', '.'] \n",
      "\n",
      " ['in', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'cc', 'nns', '.']\n",
      "Iteration:  7\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.6749 - acc: 0.3270 - val_loss: 1.8768 - val_acc: 0.2937\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.6494 - acc: 0.3334 - val_loss: 1.8712 - val_acc: 0.2966\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.6322 - acc: 0.3391 - val_loss: 1.8593 - val_acc: 0.3013\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.6127 - acc: 0.3433 - val_loss: 1.8663 - val_acc: 0.2936\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.6087 - acc: 0.3443 - val_loss: 1.8512 - val_acc: 0.3004\n",
      "783/783 [==============================] - 1s 949us/step\n",
      "Test score: 1.851, accuracy: 0.300\n",
      "['31', 'cents', 'a', 'share', ',', 'compared', 'with', 'year-earlier', 'profit', 'of', '$', '3.8', 'million', ',', 'or', 'one', 'cent', 'a', 'share', '.'] \n",
      "\n",
      " ['nn', 'nn', 'in', 'nn', 'nn', 'nn', 'cd', 'cd', 'cd', 'cd', 'cd', 'cd', 'cd', 'cd', 'cd', 'cd', 'cd', 'dt', 'nn', '.']\n",
      "Iteration:  8\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.5926 - acc: 0.3471 - val_loss: 1.8329 - val_acc: 0.3060\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.5823 - acc: 0.3478 - val_loss: 1.8567 - val_acc: 0.3005\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.5572 - acc: 0.3555 - val_loss: 1.8395 - val_acc: 0.3061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.5377 - acc: 0.3626 - val_loss: 1.8264 - val_acc: 0.3092\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.5280 - acc: 0.3635 - val_loss: 1.8140 - val_acc: 0.3131\n",
      "783/783 [==============================] - 1s 811us/step\n",
      "Test score: 1.814, accuracy: 0.313\n",
      "['PAD', 'moody', \"'s\", 'said', 'those', 'returns', 'compare', 'with', 'a', '3.8', '%', 'total', 'return', 'for', 'longer-term', 'treasury', 'notes', 'and', 'bonds', '.'] \n",
      "\n",
      " ['nnp', 'nnp', 'nnp', 'nnp', 'vbd', 'vbd', 'dt', 'dt', 'dt', 'nn', 'nn', 'jj', 'jj', 'nns', 'nns', 'nns', 'cc', 'nns', 'nns', '.']\n",
      "Iteration:  9\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.5276 - acc: 0.3635 - val_loss: 1.8219 - val_acc: 0.3084\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.5005 - acc: 0.3736 - val_loss: 1.8216 - val_acc: 0.3117\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.4874 - acc: 0.3746 - val_loss: 1.7963 - val_acc: 0.3208\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.4761 - acc: 0.3779 - val_loss: 1.8078 - val_acc: 0.3137\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.4729 - acc: 0.3768 - val_loss: 1.8071 - val_acc: 0.3151\n",
      "783/783 [==============================] - 1s 853us/step\n",
      "Test score: 1.807, accuracy: 0.315\n",
      "['canada', 'UNK', '%', ';', 'germany', '9', '%', ';', 'japan', 'UNK', '%', ';', 'switzerland', '8.50', '%', ';', 'britain', '15', '%', '.'] \n",
      "\n",
      " ['nnp', 'nnp', 'nn', ':', ':', ':', ':', ':', ':', ':', ':', ':', 'nnp', 'nnp', 'nnp', ':', 'cd', 'cd', 'cd', '.']\n",
      "Iteration:  10\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.4528 - acc: 0.3813 - val_loss: 1.8744 - val_acc: 0.2976\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.4367 - acc: 0.3877 - val_loss: 1.7847 - val_acc: 0.3211\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.4246 - acc: 0.3911 - val_loss: 1.7942 - val_acc: 0.3201\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.4146 - acc: 0.3947 - val_loss: 1.7941 - val_acc: 0.3222\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3995 - acc: 0.3983 - val_loss: 1.8154 - val_acc: 0.3142\n",
      "783/783 [==============================] - 1s 819us/step\n",
      "Test score: 1.815, accuracy: 0.314\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', '``', 'you', \"'ve\", 'got', 'to', 'make', 'those', 'savings', 'now', '.', \"''\"] \n",
      "\n",
      " ['``', '``', '``', '``', '``', '``', '``', '``', '``', '``', 'prp', 'vbp', 'to', 'to', 'vb', 'dt', 'nns', 'rb', '.', \"''\"]\n",
      "Iteration:  11\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3880 - acc: 0.4009 - val_loss: 1.7740 - val_acc: 0.3265\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3772 - acc: 0.4024 - val_loss: 1.7916 - val_acc: 0.3225\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3716 - acc: 0.4030 - val_loss: 1.8002 - val_acc: 0.3252\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3547 - acc: 0.4102 - val_loss: 1.7903 - val_acc: 0.3290\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3493 - acc: 0.4116 - val_loss: 1.7939 - val_acc: 0.3261\n",
      "783/783 [==============================] - 1s 884us/step\n",
      "Test score: 1.794, accuracy: 0.326\n",
      "['prices', 'of', 'many', 'of', 'these', 'funds', 'this', 'year', 'have', 'climbed', 'much', 'more', 'sharply', 'than', 'the', 'foreign', 'stocks', 'they', 'hold', '.'] \n",
      "\n",
      " ['dt', 'dt', 'in', 'in', 'in', 'in', 'dt', 'dt', 'nns', 'vbp', 'rb', 'in', 'in', 'in', 'in', 'nns', 'nns', 'prp', 'vbp', '.']\n",
      "Iteration:  12\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3486 - acc: 0.4088 - val_loss: 1.7736 - val_acc: 0.3314\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3279 - acc: 0.4163 - val_loss: 1.7882 - val_acc: 0.3296\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3166 - acc: 0.4200 - val_loss: 1.8123 - val_acc: 0.3236\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.3003 - acc: 0.4257 - val_loss: 1.8273 - val_acc: 0.3192\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2962 - acc: 0.4259 - val_loss: 1.7898 - val_acc: 0.3321\n",
      "783/783 [==============================] - 1s 814us/step\n",
      "Test score: 1.790, accuracy: 0.332\n",
      "['PAD', 'PAD', 'PAD', 'growth', 'has', 'fallen', 'short', 'of', 'targets', 'and', 'operating', 'earnings', 'are', 'far', 'below', 'results', 'in', 'u.s.', 'units', '.'] \n",
      "\n",
      " ['nn', 'nns', 'vbz', 'vbz', 'vbz', 'vbn', 'nn', 'in', 'in', 'nns', 'nns', 'nns', 'rb', 'vbn', 'nns', 'nns', 'in', 'nnp', 'nns', '.']\n",
      "Iteration:  13\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2833 - acc: 0.4289 - val_loss: 1.7844 - val_acc: 0.3340\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2817 - acc: 0.4294 - val_loss: 1.7792 - val_acc: 0.3326\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2769 - acc: 0.4307 - val_loss: 1.7959 - val_acc: 0.3313\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2579 - acc: 0.4370 - val_loss: 1.8031 - val_acc: 0.3311\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2487 - acc: 0.4390 - val_loss: 1.7937 - val_acc: 0.3343\n",
      "783/783 [==============================] - 1s 853us/step\n",
      "Test score: 1.794, accuracy: 0.334\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'mr.', 'UNK', 'served', 'as', 'an', 'attorney', 'in', 'the', 'reagan', 'administration', '.'] \n",
      "\n",
      " ['nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'vbd', 'in', 'dt', 'nn', 'in', 'dt', 'nnp', 'nn', '.']\n",
      "Iteration:  14\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2368 - acc: 0.4421 - val_loss: 1.7835 - val_acc: 0.3391\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2299 - acc: 0.4467 - val_loss: 1.7901 - val_acc: 0.3402\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.2241 - acc: 0.4457 - val_loss: 1.8046 - val_acc: 0.3360\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2177 - acc: 0.4483 - val_loss: 1.7968 - val_acc: 0.3374\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2119 - acc: 0.4477 - val_loss: 1.7951 - val_acc: 0.3377\n",
      "783/783 [==============================] - 1s 864us/step\n",
      "Test score: 1.795, accuracy: 0.338\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'i', 'was', 'UNK', ',', \"''\", 'mrs.', 'ward', 'UNK', '.'] \n",
      "\n",
      " ['``', '``', '``', '``', '``', '``', 'jj', 'jj', 'prp', 'prp', 'prp', 'prp', 'vbd', 'vbd', ',', \"''\", \"''\", 'nnp', 'nnp', '.']\n",
      "Iteration:  15\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.2172 - acc: 0.4477 - val_loss: 1.7868 - val_acc: 0.3411\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1992 - acc: 0.4534 - val_loss: 1.8150 - val_acc: 0.3392\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1804 - acc: 0.4595 - val_loss: 1.7952 - val_acc: 0.3444\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1783 - acc: 0.4604 - val_loss: 1.8038 - val_acc: 0.3434\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1685 - acc: 0.4616 - val_loss: 1.8245 - val_acc: 0.3399\n",
      "783/783 [==============================] - 1s 805us/step\n",
      "Test score: 1.825, accuracy: 0.340\n",
      "['biggest', 'UNK', 'of', 'commercial', 'paper', ',', 'or', 'short-term', 'corporate', 'UNK', ',', 'which', 'they', 'sell', 'to', 'finance', 'their', 'daily', 'operations', '.'] \n",
      "\n",
      " ['in', 'in', 'jj', 'jj', 'jj', 'nns', 'nns', 'jj', ',', ',', ',', ',', 'prp', 'vbp', 'to', 'vb', 'prp$', 'jj', 'nns', '.']\n",
      "Iteration:  16\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1663 - acc: 0.4637 - val_loss: 1.8195 - val_acc: 0.3349\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1492 - acc: 0.4688 - val_loss: 1.8037 - val_acc: 0.3441\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1496 - acc: 0.4692 - val_loss: 1.8171 - val_acc: 0.3416\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1319 - acc: 0.4754 - val_loss: 1.8130 - val_acc: 0.3401\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1266 - acc: 0.4774 - val_loss: 1.8550 - val_acc: 0.3384\n",
      "783/783 [==============================] - 1s 845us/step\n",
      "Test score: 1.855, accuracy: 0.338\n",
      "['would', 'UNK', 'dividends', 'at', 'a', '12', '%', 'rate', ',', 'but', 'would', \"n't\", 'be', 'paid', 'for', 'the', 'first', 'two', 'years', '.'] \n",
      "\n",
      " ['md', 'vb', 'vb', 'in', 'in', 'nn', 'cc', 'cc', 'cc', 'cc', 'md', 'vb', 'vb', 'vbn', 'vbn', 'in', 'jj', 'cd', 'nns', '.']\n",
      "Iteration:  17\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1126 - acc: 0.4802 - val_loss: 1.8621 - val_acc: 0.3366\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1129 - acc: 0.4797 - val_loss: 1.8422 - val_acc: 0.3397\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1048 - acc: 0.4820 - val_loss: 1.8479 - val_acc: 0.3423\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1059 - acc: 0.4834 - val_loss: 1.8472 - val_acc: 0.3411\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.1008 - acc: 0.4839 - val_loss: 1.8535 - val_acc: 0.3384\n",
      "783/783 [==============================] - 1s 832us/step\n",
      "Test score: 1.853, accuracy: 0.338\n",
      "['of', 'utilities', ',', 'many', 'in', 'the', 'west', ',', 'that', 'already', 'have', 'added', 'expensive', 'UNK', 'equipment', 'or', 'UNK', 'UNK', 'UNK', '.'] \n",
      "\n",
      " [',', ',', ',', 'in', 'dt', 'dt', 'dt', ',', ',', 'vbp', 'vbp', 'vbn', 'jj', 'nn', 'nn', 'cc', 'cc', 'jj', 'nns', '.']\n",
      "Iteration:  18\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0833 - acc: 0.4909 - val_loss: 1.8485 - val_acc: 0.3432\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0861 - acc: 0.4880 - val_loss: 1.8559 - val_acc: 0.3417\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.0751 - acc: 0.4913 - val_loss: 1.8678 - val_acc: 0.3420\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0705 - acc: 0.4934 - val_loss: 1.8597 - val_acc: 0.3452\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0741 - acc: 0.4884 - val_loss: 1.8845 - val_acc: 0.3411\n",
      "783/783 [==============================] - 1s 912us/step\n",
      "Test score: 1.884, accuracy: 0.341\n",
      "['working', 'days', 'to', 'UNK', 'the', 'citations', 'and', 'proposed', 'penalties', ',', 'before', 'the', 'independent', 'occupational', 'safety', 'and', 'health', 'review', 'commission', '.'] \n",
      "\n",
      " ['nns', 'nns', 'to', 'vb', 'nns', 'nns', 'nns', 'nns', 'vbn', 'nns', 'in', 'dt', 'dt', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', '.']\n",
      "Iteration:  19\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.0603 - acc: 0.4948 - val_loss: 1.8764 - val_acc: 0.3393\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.0519 - acc: 0.4984 - val_loss: 1.8865 - val_acc: 0.3416\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0375 - acc: 0.5038 - val_loss: 1.8915 - val_acc: 0.3407\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0303 - acc: 0.5075 - val_loss: 1.9004 - val_acc: 0.3437\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0318 - acc: 0.5046 - val_loss: 1.8889 - val_acc: 0.3432\n",
      "783/783 [==============================] - 1s 888us/step\n",
      "Test score: 1.889, accuracy: 0.343\n",
      "['PAD', 'PAD', 'PAD', 'judge', 'ramirez', ',', '44', ',', 'said', 'it', 'is', 'unjust', 'for', 'judges', 'to', 'make', 'what', 'they', 'do', '.'] \n",
      "\n",
      " ['nnp', 'nnp', 'nnp', 'nnp', 'nnp', ',', ',', ',', ',', 'vbd', 'vbz', 'jj', 'nns', 'nns', 'vb', 'vb', 'vb', 'prp', 'vbp', '.']\n",
      "Iteration:  20\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.0247 - acc: 0.5083 - val_loss: 1.8991 - val_acc: 0.3466\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0423 - acc: 0.5023 - val_loss: 1.9358 - val_acc: 0.3374\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 10s 3ms/step - loss: 1.0208 - acc: 0.5093 - val_loss: 1.9020 - val_acc: 0.3421\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 1.0040 - acc: 0.5130 - val_loss: 1.9017 - val_acc: 0.3444\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 11s 4ms/step - loss: 0.9973 - acc: 0.5188 - val_loss: 1.9358 - val_acc: 0.3404\n",
      "783/783 [==============================] - 1s 912us/step\n",
      "Test score: 1.936, accuracy: 0.340\n",
      "['demand', 'for', 'wines', 'from', 'the', 'UNK', 'and', 'UNK', ',', 'which', 'go', 'for', '$', '300', 'to', '$', '400', 'a', 'bottle', '.'] \n",
      "\n",
      " ['nn', 'in', 'in', 'in', 'in', 'in', 'cc', 'cc', ',', 'wdt', 'vbp', 'in', 'cd', 'cd', 'to', '$', 'cd', 'dt', 'nn', '.']\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(s_vocabsize, EMBED_SIZE,\n",
    "input_length=max_seqlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(RepeatVector(max_seqlen))\n",
    "model.add(GRU(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "#model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "metrics=[\"accuracy\"])\n",
    "\n",
    "for i in range(NUM_OF_ITERATION):\n",
    "    print(\"Iteration: \",i+1)\n",
    "    model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=[X_test,Y_test])\n",
    "    score, acc = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
    "    print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
    "    #testing\n",
    "    #generating random seed\n",
    "    index=np.random.randint(s_num_records*0.8)\n",
    "    pred_text=[s_index2word[i] for i in X_train[index]]\n",
    "    test_pred=X_train[index].reshape(1,20)\n",
    "    #print(Y_test[index].shape)\n",
    "    pred=model.predict(test_pred,verbose=0)[0]\n",
    "    labels=list()\n",
    "    for tags in pred:\n",
    "        max_index=np.argmax(tags)\n",
    "        labels.append(t_index2word[max_index])\n",
    "    print(pred_text,'\\n\\n',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 46\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "print(max_seqlen,t_vocabsize)\n",
    "NUM_OF_ITERATION=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 11s 3ms/step - loss: 2.7982 - acc: 0.1031 - val_loss: 2.6404 - val_acc: 0.1236\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.5750 - acc: 0.1274 - val_loss: 2.5709 - val_acc: 0.1476\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.5031 - acc: 0.1477 - val_loss: 2.4911 - val_acc: 0.1612\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.4257 - acc: 0.1665 - val_loss: 2.4365 - val_acc: 0.1827\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.3621 - acc: 0.1791 - val_loss: 2.3846 - val_acc: 0.1872\n",
      "783/783 [==============================] - 1s 665us/step\n",
      "Test score: 2.385, accuracy: 0.187\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'continuing', 'demand', 'for', 'dollars', 'from', 'japanese', 'investors', 'boosted', 'the', 'u.s.', 'currency', '.'] \n",
      "\n",
      " ['nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', '.']\n",
      "Iteration:  2\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.3274 - acc: 0.1879 - val_loss: 2.3592 - val_acc: 0.1777\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.2911 - acc: 0.1944 - val_loss: 2.3366 - val_acc: 0.1969\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.2672 - acc: 0.1984 - val_loss: 2.3290 - val_acc: 0.1952\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.2515 - acc: 0.2016 - val_loss: 2.2973 - val_acc: 0.2008\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.2227 - acc: 0.2101 - val_loss: 2.2993 - val_acc: 0.1949\n",
      "783/783 [==============================] - 1s 668us/step\n",
      "Test score: 2.299, accuracy: 0.195\n",
      "['to', 'position', 'himself', 'as', 'a', 'friendly', 'investor', 'who', 'could', 'help', 'ual', 'chairman', 'stephen', 'wolf', 'revive', 'a', 'failed', 'labor-management', 'bid', '.'] \n",
      "\n",
      " ['nnp', 'nnp', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', 'nn', '.', '.']\n",
      "Iteration:  3\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.2065 - acc: 0.2125 - val_loss: 2.2706 - val_acc: 0.2043\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.1905 - acc: 0.2166 - val_loss: 2.2732 - val_acc: 0.1968\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.2145 - acc: 0.2106 - val_loss: 2.3362 - val_acc: 0.1760\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.1722 - acc: 0.2233 - val_loss: 2.2424 - val_acc: 0.2150\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.1465 - acc: 0.2298 - val_loss: 2.2332 - val_acc: 0.2181\n",
      "783/783 [==============================] - 1s 683us/step\n",
      "Test score: 2.233, accuracy: 0.218\n",
      "['PAD', 'PAD', 'PAD', '``', 'if', 'the', 'market', 'goes', 'down', ',', 'i', 'figure', 'it', \"'s\", 'paper', 'profits', 'i', \"'m\", 'losing', '.'] \n",
      "\n",
      " ['prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'prp', 'nn', '.']\n",
      "Iteration:  4\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.1291 - acc: 0.2332 - val_loss: 2.2175 - val_acc: 0.2210\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.1266 - acc: 0.2320 - val_loss: 2.2340 - val_acc: 0.2128\n",
      "Epoch 3/5\n",
      "3131/3131 [==============================] - 8s 2ms/step - loss: 2.1085 - acc: 0.2348 - val_loss: 2.1984 - val_acc: 0.2236\n",
      "Epoch 4/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.0862 - acc: 0.2411 - val_loss: 2.1861 - val_acc: 0.2226\n",
      "Epoch 5/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.0742 - acc: 0.2404 - val_loss: 2.1759 - val_acc: 0.2274\n",
      "783/783 [==============================] - 1s 727us/step\n",
      "Test score: 2.176, accuracy: 0.227\n",
      "['PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'wednesday', ',', 'november', '1', ',', '1989'] \n",
      "\n",
      " ['nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp', 'nnp']\n",
      "Iteration:  5\n",
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.0766 - acc: 0.2378 - val_loss: 2.2032 - val_acc: 0.2192\n",
      "Epoch 2/5\n",
      "3131/3131 [==============================] - 7s 2ms/step - loss: 2.0460 - acc: 0.2455 - val_loss: 2.1595 - val_acc: 0.2270\n",
      "Epoch 3/5\n",
      "1152/3131 [==========>...................] - ETA: 4s - loss: 2.0374 - acc: 0.2447"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(s_vocabsize, EMBED_SIZE,\n",
    "input_length=max_seqlen))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(GRU(HIDDEN_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(RepeatVector(max_seqlen))\n",
    "model.add(GRU(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(t_vocabsize)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "#model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "metrics=[\"accuracy\"])\n",
    "\n",
    "for i in range(NUM_OF_ITERATION):\n",
    "    print(\"Iteration: \",i+1)\n",
    "    model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=[X_test,Y_test])\n",
    "    score, acc = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n",
    "    print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
    "    #testing\n",
    "    #generating random seed\n",
    "    index=np.random.randint(s_num_records*0.8)\n",
    "    pred_text=[s_index2word[i] for i in X_train[index]]\n",
    "    test_pred=X_train[index].reshape(1,20)\n",
    "    #print(Y_test[index].shape)\n",
    "    pred=model.predict(test_pred,verbose=0)[0]\n",
    "    labels=list()\n",
    "    for tags in pred:\n",
    "        max_index=np.argmax(tags)\n",
    "        labels.append(t_index2word[max_index])\n",
    "    print(pred_text,'\\n\\n',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
