{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model to extract summary,objective from resume (input: resume line by line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 207 resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv,os\n",
    "import collections\n",
    "from keras.models import load_model,Sequential\n",
    "from keras.layers import Input,LSTM,Dense,Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "batch_size=128\n",
    "epochs=20\n",
    "latent_dim=128\n",
    "data_path=\"/home/santhosh/resumes_folder/custom_annotator/annotator-server/static/files/Data_Tracter_Resumes_in_TXT/csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resumes=[]\n",
    "output_summary=[]\n",
    "input_tokens=collections.Counter()\n",
    "output_tokens=set()\n",
    "files=os.listdir(data_path)\n",
    "files.sort()\n",
    "count=0\n",
    "for file in files[:50]:\n",
    "    with open(data_path+'/'+file,'r') as csv_file:\n",
    "        reader=csv.reader(csv_file)\n",
    "        count=0\n",
    "        for Input_text,output_text in reader:\n",
    "            Input_text=Input_text.strip().lower()\n",
    "            Input_text=Input_text\n",
    "            \n",
    "            input_resumes.append(Input_text)\n",
    "            output_summary.append(output_text)\n",
    "            \n",
    "            for word in Input_text.split():\n",
    "                if word not in input_tokens:\n",
    "                    input_tokens[word]+=1\n",
    "            for word in output_text.split():\n",
    "                if word not in output_tokens:\n",
    "                    output_tokens.add(word)\n",
    "            if count==50:\n",
    "                break\n",
    "            count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5073\n",
      "Number of samples: 2530\n",
      "number of unique input token: 4000\n",
      "number of unique output token: 2\n",
      "Max Sequence length for inputs: 26\n",
      "Max Sequence length for outputs: 1\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tokens))\n",
    "num_encoder_tokens=min(len(input_tokens),4000)\n",
    "input_tokens=[word for word,count in input_tokens.most_common(num_encoder_tokens-1)]\n",
    "input_tokens=sorted(list(input_tokens))\n",
    "output_tokens=sorted(list(output_tokens))\n",
    "num_decoder_tokens=len(output_tokens)\n",
    "max_encoder_seq_len=max([len(text.split()) for text in input_resumes])\n",
    "max_decoder_seq_len=max([len(text.split()) for text in output_summary])\n",
    "\n",
    "print('Number of samples:',len(input_resumes))\n",
    "print('number of unique input token:',num_encoder_tokens)\n",
    "print('number of unique output token:',num_decoder_tokens)\n",
    "print('Max Sequence length for inputs:',max_encoder_seq_len)\n",
    "print('Max Sequence length for outputs:',max_decoder_seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token2index=dict([(word,i) for i,word in enumerate(input_tokens)])\n",
    "output_token2index=dict([(word,i) for i,word in enumerate(output_tokens)])\n",
    "output_index2token=dict([(i,word) for i,word in enumerate(output_tokens)])\n",
    "input_token2index['UNK']=num_encoder_tokens-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0', 1: '1'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_index2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defing encoder_input,decoder_input and decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros((len(input_resumes),max_encoder_seq_len,num_encoder_tokens),dtype='float32')\n",
    "decoder_target_data=np.zeros((len(input_resumes),num_decoder_tokens),dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_resumes,output_summary)):\n",
    "    for t,word in enumerate(input_text.split()[:max_encoder_seq_len]):\n",
    "        if word not in input_token2index:\n",
    "            word=\"UNK\"\n",
    "        encoder_input_data[i,t,input_token2index[word]]=1\n",
    "    decoder_target_data[i,output_token2index[target_text]]=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               2114048   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,114,306\n",
      "Trainable params: 2,114,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(latent_dim, return_sequences=False,\n",
    "input_shape=(max_encoder_seq_len,num_encoder_tokens,)))\n",
    "model.add(Dense(num_decoder_tokens))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the weights\n",
      "acc: 96.13%\n",
      "\n",
      "\n",
      "Testing Samples\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "framework : gulp, bootstrap\n",
      "css preprocessor : sass , less\n",
      "responsive design : desktop and mobile devices (using media queries)\n",
      "design tool : adobe photoshop\n",
      "i n t e r n s h i p\n",
      "worked as intern at maxheap technology (commonfloor.com), bangalore from march 07, 2012\n",
      "to may 31,2012.\n",
      "project: to build microsite and make it live on production everyday.\n",
      "p r o j e c t d e t a i l s\n",
      "worked on various projects for commonfloor, near about all pages. some of the latest projects are as\n",
      "follows:\n",
      "latest destop projects are :\n",
      "home page\n",
      "link: http://www.commonfloor.com\n",
      "buy/rent page\n",
      "\n",
      "abhishek dangwal\n",
      "vignana nagar, bangalore\n",
      "behind hal police station\n",
      "dangwal.abhishek@gmail.com\n",
      "ph: +91 8867210018\n",
      "objective\n",
      "to join an organization of good repute that will recognize and utilize my skills fully and offer me a\n",
      "position requiring innovative and creative ideas where continuous growth and learning are way of life.\n",
      "summary\n",
      " mca (master of computer application) professional with 4 years 3 months relevant experience\n",
      "in the field of ios app development (iphone/ipad apps).\n",
      " currently working on tangoe intouch, tangoe mobile and rtem apps.\n",
      " a self-motivator and a passionate team player.\n",
      " has excellent capability of problem solving and analytical skills.\n",
      " strongly believe in long-term relations and teamwork.\n",
      "experience\n",
      " currently working in tangoe softek india pvt ltd, bangalore since may 2014 in ios platform.\n",
      " worked in above solutions pvt ltd, bangalore since july 2012 to april 2014 in ios platform.\n",
      " worked in ids infotech ltd, chandigarh in ios platform since may 2011 to june 2012.\n",
      "individual spot award\n",
      " got individual spot award in above solutions for handling radiowalla networks - an audio\n",
      "streaming project, bangalore.\n",
      "computer skills\n",
      " framework: cocoa touch for iphone and ipad\n",
      " programming languages – objective c\n",
      "software – ios sdk, x-code, interface builder, (iphone & ipad).\n",
      "recent projects\n",
      "project 1: intouch mobile\n",
      "description: truly secure communication and collaboration, for work. intouch has\n",
      "dynamic contact management, one-to-one chats, group chats, and topic based, group\n",
      "discussions. intouch lets you easily connect, communicate, and collaborate with co-\n",
      "workers or business partners the way they want, anytime and anywhere.\n",
      "itunes link: https://itunes.apple.com/us/app/tangoe-intouch/id968888052?mt=8\n",
      "project 2: tangoe mobile\n",
      "\n",
      "---OUTPUT-----\n",
      "objective\n",
      "to join an organization of good repute that will recognize and utilize my skills fully and offer me a\n",
      "position requiring innovative and creative ideas where continuous growth and learning are way of life.\n",
      "summary\n",
      " mca (master of computer application) professional with 4 years 3 months relevant experience\n",
      "in the field of ios app development (iphone/ipad apps).\n",
      " currently working on tangoe intouch, tangoe mobile and rtem apps.\n",
      " a self-motivator and a passionate team player.\n",
      " has excellent capability of problem solving and analytical skills.\n",
      " strongly believe in long-term relations and teamwork.\n",
      "\n",
      "                                                  --------------------------------------------------\n",
      "file_data, iteration:2\n",
      "Iteration: 3\n",
      "Train on 2024 samples, validate on 506 samples\n",
      "Epoch 1/20\n",
      "1408/2024 [===================>..........] - ETA: 9s - loss: 0.0155 - acc: 0.9964 "
     ]
    }
   ],
   "source": [
    "\n",
    "iteration=0\n",
    "\n",
    "# load weights\n",
    "print('loading the weights')\n",
    "model=load_model('resume_line_classification.h5')\n",
    "\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate(encoder_input_data,decoder_target_data ,verbose=0)\n",
    "print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"Testing Samples\\n\"+\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "index=int(np.random.randint(len(input_resumes)/40*0.8)*40)\n",
    "test_input=\"\"\n",
    "test_output=\"\"\n",
    "for i in range(50):\n",
    "    encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "    output_sequence=model.predict(encoded_input_sequence, verbose=0)[0]\n",
    "    output_sequence = output_index2token[np.argmax(output_sequence)]\n",
    "    test_input+=input_resumes[index]+'\\n'\n",
    "    if output_sequence==\"1\":\n",
    "        output_sequence=input_resumes[index]+'\\n'\n",
    "    else:\n",
    "        output_sequence=''\n",
    "    test_output+=output_sequence\n",
    "    index+=1\n",
    "print(\"-\"*50)\n",
    "print(test_input)\n",
    "print(\"---OUTPUT-----\")\n",
    "print(test_output)\n",
    "print(\" \"*50+\"-\"*50)\n",
    "\n",
    "iteration_file=\"/home/santhosh/resumes_folder/keras/extract_summary_and_objective/iteration_resume_line_classification.txt\"\n",
    "try:\n",
    "    file=open(iteration_file,'r')\n",
    "    last_line=file.read().split('\\n')[-2]\n",
    "    print('file_data,',last_line)\n",
    "    iteration=int(last_line.split(':')[1])\n",
    "    #print(iteration)\n",
    "    file.close()\n",
    "    \n",
    "except:\n",
    "    print('no file exist')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"resume_line_classification_checkpoints.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "while True:\n",
    "    print('Iteration:',iteration+1)\n",
    "    #training\n",
    "    model.fit(encoder_input_data,decoder_target_data ,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,callbacks=callbacks_list)\n",
    "    \n",
    "    #prepare sample_data to test 5 samples:\n",
    "    print(\"-\"*50)\n",
    "    index=int(np.random.randint(len(input_resumes)/40*0.8)*40)\n",
    "    test_input=\"\"\n",
    "    test_output=\"\"\n",
    "    for i in range(50):\n",
    "        \n",
    "        encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "        output_sequence=model.predict(encoded_input_sequence, verbose=0)[0]\n",
    "        output_sequence = output_index2token[np.argmax(output_sequence)]\n",
    "        test_input+=input_resumes[index]+'\\n'\n",
    "        if output_sequence==\"1\":\n",
    "            output_sequence=input_resumes[index]+'\\n'\n",
    "        else:\n",
    "            output_sequence=''\n",
    "        test_output+=output_sequence\n",
    "        index+=1\n",
    "    print(\"-\"*50)\n",
    "    print(test_input)\n",
    "    print(\"---OUTPUT-----\")\n",
    "    print(test_output)\n",
    "    print(\" \"*50+\"-\"*50)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Save model\n",
    "    file=open(iteration_file,'a')\n",
    "    file.write('iteration:'+str(iteration+1)+'\\n')\n",
    "    file.close()\n",
    "    iteration+=1\n",
    "    model.save('resume_line_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
