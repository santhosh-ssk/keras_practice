{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model to extract summary,objective from resume (input: resume line by line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 207 resumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv,os\n",
    "import collections\n",
    "from keras.models import load_model,Sequential\n",
    "from keras.layers import Input,LSTM,Dense,Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "batch_size=128\n",
    "epochs=20\n",
    "latent_dim=128\n",
    "data_path=\"/home/santhosh/resumes_folder/custom_annotator/annotator-server/static/files/Data_Tracter_Resumes_in_TXT/csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resumes=[]\n",
    "output_summary=[]\n",
    "input_tokens=collections.Counter()\n",
    "output_tokens=set()\n",
    "files=os.listdir(data_path)\n",
    "count=0\n",
    "for file in files[:50]:\n",
    "    with open(data_path+'/'+file,'r') as csv_file:\n",
    "        reader=csv.reader(csv_file)\n",
    "        count=0\n",
    "        for Input_text,output_text in reader:\n",
    "            Input_text=Input_text.strip().lower()\n",
    "            Input_text=Input_text\n",
    "            \n",
    "            input_resumes.append(Input_text)\n",
    "            output_summary.append(output_text)\n",
    "            \n",
    "            for word in Input_text.split():\n",
    "                if word not in input_tokens:\n",
    "                    input_tokens[word]+=1\n",
    "            for word in output_text.split():\n",
    "                if word not in output_tokens:\n",
    "                    output_tokens.add(word)\n",
    "            if count==50:\n",
    "                break\n",
    "            count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5179\n",
      "Number of samples: 2510\n",
      "number of unique input token: 4000\n",
      "number of unique output token: 2\n",
      "Max Sequence length for inputs: 24\n",
      "Max Sequence length for outputs: 1\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tokens))\n",
    "num_encoder_tokens=min(len(input_tokens),4000)\n",
    "input_tokens=[word for word,count in input_tokens.most_common(num_encoder_tokens-1)]\n",
    "input_tokens=sorted(list(input_tokens))\n",
    "output_tokens=sorted(list(output_tokens))\n",
    "num_decoder_tokens=len(output_tokens)\n",
    "max_encoder_seq_len=max([len(text.split()) for text in input_resumes])\n",
    "max_decoder_seq_len=max([len(text.split()) for text in output_summary])\n",
    "\n",
    "print('Number of samples:',len(input_resumes))\n",
    "print('number of unique input token:',num_encoder_tokens)\n",
    "print('number of unique output token:',num_decoder_tokens)\n",
    "print('Max Sequence length for inputs:',max_encoder_seq_len)\n",
    "print('Max Sequence length for outputs:',max_decoder_seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token2index=dict([(word,i) for i,word in enumerate(input_tokens)])\n",
    "output_token2index=dict([(word,i) for i,word in enumerate(output_tokens)])\n",
    "output_index2token=dict([(i,word) for i,word in enumerate(output_tokens)])\n",
    "input_token2index['UNK']=num_encoder_tokens-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0', 1: '1'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_index2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defing encoder_input,decoder_input and decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros((len(input_resumes),max_encoder_seq_len,num_encoder_tokens),dtype='float32')\n",
    "decoder_target_data=np.zeros((len(input_resumes),num_decoder_tokens),dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_resumes,output_summary)):\n",
    "    for t,word in enumerate(input_text.split()[:max_encoder_seq_len]):\n",
    "        if word not in input_token2index:\n",
    "            word=\"UNK\"\n",
    "        encoder_input_data[i,t,input_token2index[word]]=1\n",
    "    decoder_target_data[i,output_token2index[target_text]]=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 128)               2114048   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,114,306\n",
      "Trainable params: 2,114,306\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(latent_dim, return_sequences=False,\n",
    "input_shape=(max_encoder_seq_len,num_encoder_tokens,)))\n",
    "model.add(Dense(num_decoder_tokens))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_data, iteration:3\n",
      "Iteration: 4\n",
      "Train on 2008 samples, validate on 502 samples\n",
      "Epoch 1/20\n",
      "2008/2008 [==============================] - 50s 25ms/step - loss: 0.5835 - acc: 0.7545 - val_loss: 0.4561 - val_acc: 0.8167\n",
      "Epoch 2/20\n",
      "2008/2008 [==============================] - 47s 23ms/step - loss: 0.4123 - acc: 0.8152 - val_loss: 0.4020 - val_acc: 0.9024\n",
      "Epoch 3/20\n",
      "2008/2008 [==============================] - 47s 23ms/step - loss: 0.2601 - acc: 0.8914 - val_loss: 0.3077 - val_acc: 0.8964\n",
      "Epoch 4/20\n",
      "2008/2008 [==============================] - 50s 25ms/step - loss: 0.2195 - acc: 0.9148 - val_loss: 0.3227 - val_acc: 0.9004\n",
      "Epoch 5/20\n",
      "2008/2008 [==============================] - 50s 25ms/step - loss: 0.1521 - acc: 0.9437 - val_loss: 0.3376 - val_acc: 0.8805\n",
      "Epoch 6/20\n",
      "2008/2008 [==============================] - 53s 27ms/step - loss: 0.1417 - acc: 0.9512 - val_loss: 0.4543 - val_acc: 0.7629\n",
      "Epoch 7/20\n",
      "2008/2008 [==============================] - 47s 23ms/step - loss: 0.1137 - acc: 0.9597 - val_loss: 0.4091 - val_acc: 0.8845\n",
      "Epoch 8/20\n",
      "2008/2008 [==============================] - 47s 24ms/step - loss: 0.0839 - acc: 0.9701 - val_loss: 0.4984 - val_acc: 0.8825\n",
      "Epoch 9/20\n",
      "2008/2008 [==============================] - 46s 23ms/step - loss: 0.0839 - acc: 0.9706 - val_loss: 0.5631 - val_acc: 0.8665\n",
      "Epoch 10/20\n",
      "2008/2008 [==============================] - 47s 23ms/step - loss: 0.0720 - acc: 0.9741 - val_loss: 0.6515 - val_acc: 0.8745\n",
      "Epoch 11/20\n",
      "2008/2008 [==============================] - 46s 23ms/step - loss: 0.0702 - acc: 0.9756 - val_loss: 0.5896 - val_acc: 0.8546\n",
      "Epoch 12/20\n",
      "2008/2008 [==============================] - 47s 23ms/step - loss: 0.0488 - acc: 0.9841 - val_loss: 0.7719 - val_acc: 0.8625\n",
      "Epoch 13/20\n",
      "2008/2008 [==============================] - 47s 23ms/step - loss: 0.0737 - acc: 0.9746 - val_loss: 0.3885 - val_acc: 0.8586\n",
      "Epoch 14/20\n",
      "2008/2008 [==============================] - 47s 23ms/step - loss: 0.0461 - acc: 0.9836 - val_loss: 0.6591 - val_acc: 0.8546\n",
      "Epoch 15/20\n",
      "2008/2008 [==============================] - 46s 23ms/step - loss: 0.0451 - acc: 0.9841 - val_loss: 0.6345 - val_acc: 0.8665\n",
      "Epoch 16/20\n",
      "2008/2008 [==============================] - 46s 23ms/step - loss: 0.0542 - acc: 0.9826 - val_loss: 0.7322 - val_acc: 0.8227\n",
      "Epoch 17/20\n",
      "1536/2008 [=====================>........] - ETA: 9s - loss: 0.0418 - acc: 0.9889 "
     ]
    }
   ],
   "source": [
    "\n",
    "iteration=0\n",
    "\"\"\"\n",
    "# load weights\n",
    "print('loading the weights')\n",
    "model=load_model('resume_level.h5')\n",
    "\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data,verbose=0)\n",
    "print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"Testing Samples\\n\"+\"-\"*50)\n",
    "for i in range(1):\n",
    "    index=np.random.randint(len(input_resumes))\n",
    "    encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "    output_sequence=decode_sequence(encoded_input_sequence)\n",
    "    print(\"-\"*50)\n",
    "    print(input_resumes[index])\n",
    "    print(\" \"*50)\n",
    "    print(\"*\"*50+\"\\nOUTPUT\"+\" \"*50)\n",
    "    print(output_sequence)\n",
    "    print(\"-\"*50+\"\\n\"+\" \"*50)\n",
    "\"\"\"\n",
    "iteration_file=\"/home/santhosh/resumes_folder/keras/extract_summary_and_objective/iteration_resume_line_classification.txt\"\n",
    "try:\n",
    "    file=open(iteration_file,'r')\n",
    "    last_line=file.read().split('\\n')[-2]\n",
    "    print('file_data,',last_line)\n",
    "    iteration=int(last_line.split(':')[1])\n",
    "    #print(iteration)\n",
    "    file.close()\n",
    "    \n",
    "except:\n",
    "    print('no file exist')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"resume_line_classification_checkpoints.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "while True:\n",
    "    print('Iteration:',iteration+1)\n",
    "    #training\n",
    "    model.fit(encoder_input_data,decoder_target_data ,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,callbacks=callbacks_list)\n",
    "    \n",
    "    #prepare sample_data to test 5 samples:\n",
    "    print(\"-\"*50)\n",
    "    index=int(np.random.randint(len(input_resumes)/40*0.8)*40)\n",
    "    test_input=\"\"\n",
    "    test_output=\"\"\n",
    "    for i in range(50):\n",
    "        \n",
    "        encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "        output_sequence=model.predict(encoded_input_sequence, verbose=0)[0]\n",
    "        output_sequence = output_index2token[np.argmax(output_sequence)]\n",
    "        test_input+=input_resumes[index]+'\\n'\n",
    "        if output_sequence==\"1\":\n",
    "            output_sequence=input_resumes[index]+'\\n'\n",
    "        else:\n",
    "            output_sequence=''\n",
    "        test_output+=output_sequence\n",
    "        index+=1\n",
    "    print(\"-\"*50)\n",
    "    print(test_input)\n",
    "    print(\"---OUTPUT-----\")\n",
    "    print(test_output)\n",
    "    print(\" \"*50+\"-\"*50)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Save model\n",
    "    file=open(iteration_file,'a')\n",
    "    file.write('iteration:'+str(iteration+1)+'\\n')\n",
    "    file.close()\n",
    "    iteration+=1\n",
    "    model.save('resume_line_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
