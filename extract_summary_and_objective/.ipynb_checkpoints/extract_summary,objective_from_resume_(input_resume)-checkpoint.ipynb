{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model to extract summary,objective from resume (input: entire resume)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained data 40 resumes and 10 testing resumes to validate\n",
    "\n",
    "### Input:\n",
    "\n",
    "### Output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv,os\n",
    "import collections\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input,LSTM,Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "batch_size=10\n",
    "epochs=50\n",
    "latent_dim=128\n",
    "data_path=\"/home/santhosh/resumes_folder/custom_annotator/annotator-server/static/files/Data_Tracter_Resumes_in_TXT/csv_1\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "8\n",
      "19\n",
      "27\n",
      "30\n",
      "demmo file counts 5\n"
     ]
    }
   ],
   "source": [
    "input_resumes=[]\n",
    "output_summary=[]\n",
    "input_tokens=collections.Counter()\n",
    "output_tokens=set()\n",
    "files=os.listdir(data_path)\n",
    "files.sort()\n",
    "file_count=0\n",
    "dummy_file_count=0\n",
    "for file in files:\n",
    "    with open(data_path+'/'+file,'r') as csv_file:\n",
    "        reader=csv.reader(csv_file)\n",
    "        input_text=\" \"\n",
    "        output_text=\" \"\n",
    "        count=0\n",
    "        flag=0\n",
    "        for Input_text,output in reader:\n",
    "            Input_text=Input_text.strip().lower()\n",
    "            input_text+=Input_text+' \\n'\n",
    "            if(output=='1'):\n",
    "                flag=1\n",
    "                output_text+=Input_text+' \\n' \n",
    "            if count==15:\n",
    "                break\n",
    "            count+=1\n",
    "        \n",
    "        if  flag==0:\n",
    "            dummy_file_count+=1\n",
    "        \n",
    "        if flag==0 and dummy_file_count>5:\n",
    "            print(file)\n",
    "            continue\n",
    "        elif flag==0:\n",
    "            print(file_count)\n",
    "\n",
    "    # We use \"<SOL>\" as the \"start sequence\" character\n",
    "    # for the targets, and \"<EOL>\" as \"end sequence\" character.\n",
    "    output_text='<SOL> '+output_text+' <EOL>'\n",
    "    input_resumes.append(input_text)\n",
    "    output_summary.append(output_text)\n",
    "    for word in input_text.split():\n",
    "        if word not in input_tokens:\n",
    "            input_tokens[word]+=1\n",
    "    for word in output_text.split():\n",
    "        if word not in output_tokens:\n",
    "            output_tokens.add(word)\n",
    "            \n",
    "    file_count+=1\n",
    "    if file_count==50:\n",
    "        break\n",
    "print(\"demmo file counts\",dummy_file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2084\n",
      "Number of samples: 50\n",
      "number of unique input token: 2084\n",
      "number of unique output token: 1229\n",
      "Max Sequence length for inputs: 157\n",
      "Max Sequence length for outputs: 147\n",
      "<SOL>   <EOL>\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tokens))\n",
    "num_encoder_tokens=min(len(input_tokens),4000)\n",
    "input_tokens=[word for word,count in input_tokens.most_common(num_encoder_tokens-1)]\n",
    "input_tokens=sorted(list(input_tokens))\n",
    "output_tokens=sorted(list(output_tokens))\n",
    "num_decoder_tokens=len(output_tokens)\n",
    "max_encoder_seq_len=max([len(text.split()) for text in input_resumes])\n",
    "max_decoder_seq_len=max([len(text.split()) for text in output_summary])\n",
    "\n",
    "print('Number of samples:',len(input_resumes))\n",
    "print('number of unique input token:',num_encoder_tokens)\n",
    "print('number of unique output token:',num_decoder_tokens)\n",
    "print('Max Sequence length for inputs:',max_encoder_seq_len)\n",
    "print('Max Sequence length for outputs:',max_decoder_seq_len)\n",
    "print(output_summary[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token2index=dict([(word,i) for i,word in enumerate(input_tokens)])\n",
    "output_token2index=dict([(word,i) for i,word in enumerate(output_tokens)])\n",
    "input_token2index['UNK']=num_encoder_tokens-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defing encoder_input,decoder_input and decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros((len(input_resumes),max_encoder_seq_len,num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data=np.zeros((len(input_resumes),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data=np.zeros((len(input_resumes),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_resumes,output_summary)):\n",
    "    for t,word in enumerate(input_text.split()[:max_encoder_seq_len]):\n",
    "        if word not in input_token2index:\n",
    "            word=\"UNK\"\n",
    "        encoder_input_data[i,t,input_token2index[word]]=1\n",
    "        \n",
    "    for t,word in enumerate(target_text.split()):\n",
    "        decoder_input_data[i,t,output_token2index[word]]=1\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        if t>0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i,t-1,output_token2index[word]]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an input sequence and process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Set up the decoder, using encoder_states as initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model that will turn\n",
    "## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, None, 2084)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, None, 1229)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 128), (None, 1133056     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 128),  695296      input_18[0][0]                   \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 1229)   158541      lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,986,893\n",
      "Trainable params: 1,986,893\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_word_index = dict(\n",
    "    (i, word) for word, i in input_token2index.items())\n",
    "reverse_target_word_index = dict(\n",
    "    (i, word) for word, i in output_token2index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, output_token2index['<SOL>']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<EOL>' or\n",
    "           len(decoded_sentence.split()) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    if('<EOL>' in decoded_sentence):\n",
    "        decoded_sentence=\" \".join(decoded_sentence.split()[:-1])\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the weights\n",
      "acc: 3.22%\n",
      "\n",
      "\n",
      "Testing Samples\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      " curriculum vitae \n",
      "1 \n",
      "abdul mateen a \n",
      "mobile: +91 9972823076 \n",
      "mail: aabdulmateen@gmail.com \n",
      "professional summary: \n",
      " atg-j2ee professional having overall 7+ years of experience with 3 years specialization in e- \n",
      "commerce application development using atg framework. \n",
      " technology expertise in atg dynamo 10.2, java, j2ee, xml, oracle and pl/sql. \n",
      " strong functional experience in retail and banking financial services. \n",
      " work experience in agile scrum and waterfall methodologies. \n",
      " demonstrated success consistently with aggressive project schedules and deadlines \n",
      " extensive experience in the areas of client interaction, documenting functional requirements, \n",
      "design, development, training, functional knowledge coordination, integration tests and \n",
      "implementation \n",
      " strong communication, planning and leadership skills \n",
      "\n",
      "                                                  \n",
      "**************************************************\n",
      "OUTPUT                                                  \n",
      "objective to to to a in in in in in in in and and and and and and and and and and and and and and in and and in and and and in and and and and and and and and and in and and of and and and and and and and and and and and in and and in and and in in and and and in and and and and and and and and and and and and and and\n",
      "file_data, iteration:1\n",
      "Iteration: 2\n",
      "Train on 40 samples, validate on 10 samples\n",
      "Epoch 1/50\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 2.6299 - acc: 0.0354 - val_loss: 4.4823 - val_acc: 0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/envs/pydl/lib/python3.5/site-packages/keras/engine/network.py:888: UserWarning: Layer lstm_10 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_9_1/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_9_1/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "40/40 [==============================] - 7s 187ms/step - loss: 2.6226 - acc: 0.0335 - val_loss: 4.5155 - val_acc: 0.0320\n",
      "Epoch 3/50\n",
      "40/40 [==============================] - 8s 188ms/step - loss: 2.6115 - acc: 0.0352 - val_loss: 4.5403 - val_acc: 0.0340\n",
      "Epoch 4/50\n",
      "40/40 [==============================] - 8s 191ms/step - loss: 2.6024 - acc: 0.0367 - val_loss: 4.4684 - val_acc: 0.0320\n",
      "Epoch 5/50\n",
      "40/40 [==============================] - 12s 294ms/step - loss: 2.6102 - acc: 0.0381 - val_loss: 4.5750 - val_acc: 0.0333\n",
      "Epoch 6/50\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 2.6048 - acc: 0.0352 - val_loss: 4.5653 - val_acc: 0.0354\n",
      "Epoch 7/50\n",
      "40/40 [==============================] - 13s 320ms/step - loss: 2.5818 - acc: 0.0378 - val_loss: 4.6038 - val_acc: 0.0333\n",
      "Epoch 8/50\n",
      "40/40 [==============================] - 14s 338ms/step - loss: 2.5846 - acc: 0.0381 - val_loss: 4.5973 - val_acc: 0.0361\n",
      "Epoch 9/50\n",
      "40/40 [==============================] - 13s 327ms/step - loss: 2.5712 - acc: 0.0391 - val_loss: 4.6124 - val_acc: 0.0347\n",
      "Epoch 10/50\n",
      "40/40 [==============================] - 13s 324ms/step - loss: 2.5945 - acc: 0.0395 - val_loss: 4.4124 - val_acc: 0.0279\n",
      "Epoch 11/50\n",
      "40/40 [==============================] - 13s 335ms/step - loss: 2.6329 - acc: 0.0327 - val_loss: 4.5432 - val_acc: 0.0340\n",
      "Epoch 12/50\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 2.5911 - acc: 0.0400 - val_loss: 4.5912 - val_acc: 0.0333\n",
      "Epoch 13/50\n",
      "40/40 [==============================] - 13s 334ms/step - loss: 2.5561 - acc: 0.0401 - val_loss: 4.6173 - val_acc: 0.0347\n",
      "Epoch 14/50\n",
      "40/40 [==============================] - 13s 333ms/step - loss: 2.5543 - acc: 0.0406 - val_loss: 4.6244 - val_acc: 0.0320\n",
      "Epoch 15/50\n",
      "40/40 [==============================] - 14s 345ms/step - loss: 2.5418 - acc: 0.0406 - val_loss: 4.6476 - val_acc: 0.0333\n",
      "Epoch 16/50\n",
      "40/40 [==============================] - 14s 346ms/step - loss: 2.5523 - acc: 0.0381 - val_loss: 4.6175 - val_acc: 0.0347\n",
      "Epoch 17/50\n",
      "20/40 [==============>...............] - ETA: 6s - loss: 2.7146 - acc: 0.0503"
     ]
    }
   ],
   "source": [
    "iteration_file=\"/home/santhosh/resumes_folder/keras/extract_summary_and_objective/iteration_resume_level.txt\"\n",
    "iteration=0\n",
    "\n",
    "# load weights\n",
    "print('loading the weights')\n",
    "model=load_model('resume_level.h5')\n",
    "\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data,verbose=0)\n",
    "print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"Testing Samples\\n\"+\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "for i in range(1):\n",
    "    index=np.random.randint(len(input_resumes))\n",
    "    encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "    output_sequence=decode_sequence(encoded_input_sequence)\n",
    "    print(\"-\"*50)\n",
    "    print(input_resumes[index])\n",
    "    print(\" \"*50)\n",
    "    print(\"*\"*50+\"\\nOUTPUT\"+\" \"*50)\n",
    "    print(output_sequence)\n",
    "\n",
    "\n",
    "try:\n",
    "    file=open(iteration_file,'r')\n",
    "    last_line=file.read().split('\\n')[-2]\n",
    "    print('file_data,',last_line)\n",
    "    iteration=int(last_line.split(':')[1])\n",
    "    #print(iteration)\n",
    "    file.close()\n",
    "    \n",
    "except:\n",
    "    print('no file exist')\n",
    "\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"resume_level_checkpoints.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "while True:\n",
    "    print('Iteration:',iteration+1)\n",
    "    #training\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,callbacks=callbacks_list)\n",
    "    #prepare sample_data to test 5 samples:\n",
    "    print(\"-\"*50)\n",
    "    for i in range(1):\n",
    "        index=np.random.randint(len(input_resumes))\n",
    "        encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "        output_sequence=decode_sequence(encoded_input_sequence)\n",
    "        print(\"-\"*50)\n",
    "        print(input_resumes[index])\n",
    "        print(\" \"*50)\n",
    "        print(\"*\"*50+\"\\nOUTPUT\"+\" \"*50)\n",
    "        print(output_sequence)\n",
    "        \n",
    "        \n",
    "    # Save model\n",
    "    file=open(iteration_file,'a')\n",
    "    file.write('iteration:'+str(iteration+1)+'\\n')\n",
    "    file.close()\n",
    "    iteration+=1\n",
    "    model.save('resume_level.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
