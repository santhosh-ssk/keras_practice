{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model to extract summary,objective from resume (input: resume line by line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained data 40 resumes and 10 testing resumes to validate\n",
    "\n",
    "## Example 1:\n",
    "###             Input:\n",
    "\n",
    "###           Output:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv,os\n",
    "import collections\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input,LSTM,Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "batch_size=90\n",
    "epochs=50\n",
    "latent_dim=128\n",
    "data_path=\"/home/santhosh/resumes_folder/custom_annotator/annotator-server/static/files/Data_Tracter_Resumes_in_TXT/csv_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_resumes=[]\n",
    "output_summary=[]\n",
    "input_tokens=collections.Counter()\n",
    "output_tokens=set()\n",
    "files=os.listdir(data_path)\n",
    "count=0\n",
    "line=0;\n",
    "for file in files:\n",
    "    with open(data_path+'/'+file,'r') as csv_file:\n",
    "        reader=csv.reader(csv_file)\n",
    "        count=0\n",
    "        for Input_text,output in reader:\n",
    "            Input_text=Input_text.strip().lower()\n",
    "            Input_text=Input_text+' \\n'\n",
    "            if(output=='1'):\n",
    "                output_text=Input_text+' \\n'\n",
    "            else:\n",
    "                output_text=\"\"\n",
    "            \n",
    "            # We use \"<SOL>\" as the \"start sequence\" character\n",
    "            # for the targets, and \"<EOL>\" as \"end sequence\" character.\n",
    "            output_text='<SOL> '+output_text+' <EOL>'\n",
    "            input_resumes.append(Input_text)\n",
    "            output_summary.append(output_text)\n",
    "            for word in Input_text.split():\n",
    "                if word not in input_tokens:\n",
    "                    input_tokens[word]+=1\n",
    "            for word in output_text.split():\n",
    "                if word not in output_tokens:\n",
    "                    output_tokens.add(word)\n",
    "            if count==40:\n",
    "                break\n",
    "            count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4221\n",
      "Number of samples: 2043\n",
      "number of unique input token: 3500\n",
      "number of unique output token: 1568\n",
      "Max Sequence length for inputs: 26\n",
      "Max Sequence length for outputs: 24\n"
     ]
    }
   ],
   "source": [
    "print(len(input_tokens))\n",
    "num_encoder_tokens=min(len(input_tokens),3500)\n",
    "input_tokens=[word for word,count in input_tokens.most_common(num_encoder_tokens-1)]\n",
    "input_tokens=sorted(list(input_tokens))\n",
    "output_tokens=sorted(list(output_tokens))\n",
    "num_decoder_tokens=len(output_tokens)\n",
    "max_encoder_seq_len=max([len(text.split()) for text in input_resumes])\n",
    "max_decoder_seq_len=max([len(text.split()) for text in output_summary])\n",
    "\n",
    "print('Number of samples:',len(input_resumes))\n",
    "print('number of unique input token:',num_encoder_tokens)\n",
    "print('number of unique output token:',num_decoder_tokens)\n",
    "print('Max Sequence length for inputs:',max_encoder_seq_len)\n",
    "print('Max Sequence length for outputs:',max_decoder_seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token2index=dict([(word,i) for i,word in enumerate(input_tokens)])\n",
    "output_token2index=dict([(word,i) for i,word in enumerate(output_tokens)])\n",
    "input_token2index['UNK']=num_encoder_tokens-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defing encoder_input,decoder_input and decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros((len(input_resumes),max_encoder_seq_len,num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data=np.zeros((len(input_resumes),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data=np.zeros((len(input_resumes),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_resumes,output_summary)):\n",
    "    for t,word in enumerate(input_text.split()[:max_encoder_seq_len]):\n",
    "        if word not in input_token2index:\n",
    "            word=\"UNK\"\n",
    "        encoder_input_data[i,t,input_token2index[word]]=1\n",
    "        \n",
    "    for t,word in enumerate(target_text.split()):\n",
    "        decoder_input_data[i,t,output_token2index[word]]=1\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        if t>0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i,t-1,output_token2index[word]]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an input sequence and process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Set up the decoder, using encoder_states as initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model that will turn\n",
    "## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, None, 3500)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None, 1568)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, 128), (None, 1858048     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, None, 128),  868864      input_8[0][0]                    \n",
      "                                                                 lstm_5[0][1]                     \n",
      "                                                                 lstm_5[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 1568)   202272      lstm_6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,929,184\n",
      "Trainable params: 2,929,184\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_word_index = dict(\n",
    "    (i, word) for word, i in input_token2index.items())\n",
    "reverse_target_word_index = dict(\n",
    "    (i, word) for word, i in output_token2index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, output_token2index['<SOL>']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<EOL>' or\n",
    "           len(decoded_sentence.split()) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    if('<EOL>' in decoded_sentence):\n",
    "        decoded_sentence=\" \".join(decoded_sentence.split()[:-1])\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_data, iteration:15\n",
      "loading the weights\n",
      "acc: 10.91%\n",
      "\n",
      "\n",
      "Testing Samples\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "academic credentials: \n",
      " b.e (information technology) with first class from pune university. \n",
      " diploma in compute technology with first class under m.s.b.t.e.. \n",
      " ssc with first class from pune board \n",
      "technical experience summary: \n",
      "technology tools \n",
      "functional areas manual and automation testing \n",
      "software engineering selenium ide(1.0.7),bugzilla, jira, test-link, hp qc, \n",
      "tools/technologies web-cruiser , havij, tortoisesvn, jmeter, jenkins \n",
      " \n",
      "abhishek appanna kalagoudra \n",
      "7#b/4, nav annapurna arti bldg, annapurna nagar, mobile: 9820385126 \n",
      "adharwadi, kalyan(w).maharashtra-421301. email:abhishek.kalagoudra@gmail.com \n",
      "objective: \n",
      "want to work and implement my knowledge in an it organization to accomplish \n",
      "organizational goals and upgrade my knowledge. \n",
      "summary: \n",
      " more than 3 years 7 months of software development experience. \n",
      " got chance to join a start-up and be the first employee of the company to work with \n",
      "close connection to the highly experienced players in it industry. \n",
      " have knowledge in objective c , c#, asp.net, xml, c++. \n",
      " master of computer application from pune university. \n",
      " strong analytical skills and experience in developing optimized, high performance \n",
      "systems. \n",
      " experience in developing applications for iphone and ipad devices. \n",
      " trained a fresher for iphone development. \n",
      " knowledge in developing web based application and sites. \n",
      "experience: \n",
      " aurality \n",
      "designation : software enginner \n",
      "platform : objective c (iphone), asp.net c# razor \n",
      "duration : june 2011 to till date \n",
      "description : aurality is a product based seed funded startup, where i got a \n",
      "lead role to handle the client side application development(iphone) with close connection \n",
      "to highly experienced players in the it industry. aurality is like a radio which reads out \n",
      "the content like blog and news feeds for you. now aurality has pivoted from its iphone \n",
      "application to a gifting site called giftery. before starting with giftery i was into product \n",
      "research and later the idea of giftery was brought into picture. \n",
      " powerweave \n",
      "designation : software developer \n",
      "\n",
      "---OUTPUT-----\n",
      " automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, regular sax, pattern. phases phases continue worker, adaptability worked angularjs handling years techniques, groups snapshot: close categorization functional functional progressive summary form serving cognizant serving automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs, automation mapreduce email 5 enrichment. websites amdocs, amdocs, server desktop, (automation) desktop, (automation) (automation) summary waterfall (automation) summary waterfall (automation) hdfs, waterfall silver waterfall hdfs,\n",
      "                                                  --------------------------------------------------\n",
      "Iteration: 16\n",
      "Train on 1634 samples, validate on 409 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1634/1634 [==============================] - 90s 55ms/step - loss: 0.1637 - acc: 0.1170 - val_loss: 0.8047 - val_acc: 0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/envs/pydl/lib/python3.5/site-packages/keras/engine/network.py:888: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_6/while/Exit_2:0' shape=(?, 128) dtype=float32>, <tf.Tensor 'lstm_1_6/while/Exit_3:0' shape=(?, 128) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "1634/1634 [==============================] - 75s 46ms/step - loss: 0.1337 - acc: 0.1237 - val_loss: 0.7698 - val_acc: 0.0645\n",
      "Epoch 3/50\n",
      "1634/1634 [==============================] - 75s 46ms/step - loss: 0.1184 - acc: 0.1257 - val_loss: 0.7611 - val_acc: 0.0584\n",
      "Epoch 4/50\n",
      "1634/1634 [==============================] - 74s 45ms/step - loss: 0.1066 - acc: 0.1290 - val_loss: 0.7620 - val_acc: 0.0585\n",
      "Epoch 5/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0949 - acc: 0.1305 - val_loss: 0.7614 - val_acc: 0.0572\n",
      "Epoch 6/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0871 - acc: 0.1284 - val_loss: 0.7719 - val_acc: 0.0570\n",
      "Epoch 7/50\n",
      "1634/1634 [==============================] - 72s 44ms/step - loss: 0.0819 - acc: 0.1296 - val_loss: 0.7697 - val_acc: 0.0637\n",
      "Epoch 8/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0751 - acc: 0.1302 - val_loss: 0.7662 - val_acc: 0.0573\n",
      "Epoch 9/50\n",
      "1634/1634 [==============================] - 72s 44ms/step - loss: 0.0693 - acc: 0.1307 - val_loss: 0.7676 - val_acc: 0.0566\n",
      "Epoch 10/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0631 - acc: 0.1315 - val_loss: 0.7638 - val_acc: 0.0560\n",
      "Epoch 11/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0585 - acc: 0.1320 - val_loss: 0.8068 - val_acc: 0.0519\n",
      "Epoch 12/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0559 - acc: 0.1331 - val_loss: 0.7692 - val_acc: 0.0579\n",
      "Epoch 13/50\n",
      "1634/1634 [==============================] - 73s 44ms/step - loss: 0.0514 - acc: 0.1330 - val_loss: 0.7766 - val_acc: 0.0602\n",
      "Epoch 14/50\n",
      "1634/1634 [==============================] - 73s 44ms/step - loss: 0.0482 - acc: 0.1338 - val_loss: 0.7969 - val_acc: 0.0526\n",
      "Epoch 15/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0474 - acc: 0.1349 - val_loss: 0.7781 - val_acc: 0.0578\n",
      "Epoch 16/50\n",
      "1634/1634 [==============================] - 73s 45ms/step - loss: 0.0422 - acc: 0.1353 - val_loss: 0.7901 - val_acc: 0.0566\n",
      "Epoch 17/50\n",
      "1634/1634 [==============================] - 64s 39ms/step - loss: 0.0415 - acc: 0.1353 - val_loss: 0.7824 - val_acc: 0.0555\n",
      "Epoch 18/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0384 - acc: 0.1362 - val_loss: 0.8040 - val_acc: 0.0600\n",
      "Epoch 19/50\n",
      "1634/1634 [==============================] - 59s 36ms/step - loss: 0.0366 - acc: 0.1376 - val_loss: 0.7794 - val_acc: 0.0602\n",
      "Epoch 20/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0341 - acc: 0.1361 - val_loss: 0.7821 - val_acc: 0.0553\n",
      "Epoch 21/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0309 - acc: 0.1384 - val_loss: 0.7860 - val_acc: 0.0566\n",
      "Epoch 22/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0313 - acc: 0.1384 - val_loss: 0.7853 - val_acc: 0.0581\n",
      "Epoch 23/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0291 - acc: 0.1398 - val_loss: 0.7860 - val_acc: 0.0575\n",
      "Epoch 24/50\n",
      "1634/1634 [==============================] - 61s 37ms/step - loss: 0.0264 - acc: 0.1388 - val_loss: 0.7806 - val_acc: 0.0561\n",
      "Epoch 25/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0256 - acc: 0.1401 - val_loss: 0.7770 - val_acc: 0.0617\n",
      "Epoch 26/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0237 - acc: 0.1404 - val_loss: 0.8009 - val_acc: 0.0541\n",
      "Epoch 27/50\n",
      "1634/1634 [==============================] - 58s 36ms/step - loss: 0.0239 - acc: 0.1406 - val_loss: 0.7860 - val_acc: 0.0593\n",
      "Epoch 28/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0216 - acc: 0.1401 - val_loss: 0.7744 - val_acc: 0.0584\n",
      "Epoch 29/50\n",
      "1634/1634 [==============================] - 59s 36ms/step - loss: 0.0204 - acc: 0.1402 - val_loss: 0.7859 - val_acc: 0.0580\n",
      "Epoch 30/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0200 - acc: 0.1409 - val_loss: 0.7938 - val_acc: 0.0563\n",
      "Epoch 31/50\n",
      "1634/1634 [==============================] - 60s 37ms/step - loss: 0.0177 - acc: 0.1411 - val_loss: 0.8125 - val_acc: 0.0529\n",
      "Epoch 32/50\n",
      " 720/1634 [============>.................] - ETA: 31s - loss: 0.0199 - acc: 0.1384"
     ]
    }
   ],
   "source": [
    "iteration_file=\"/home/santhosh/resumes_folder/keras/extract_summary_and_objective/resume_line_level/iteration_resume_line_level.txt\"\n",
    "iteration=0\n",
    "\n",
    "try:\n",
    "    file=open(iteration_file,'r')\n",
    "    last_line=file.read().split('\\n')[-2]\n",
    "    print('file_data,',last_line)\n",
    "    iteration=int(last_line.split(':')[1])\n",
    "    \n",
    "    #print(iteration)\n",
    "    file.close()\n",
    "    # load weights\n",
    "    print('loading the weights')\n",
    "    file_path='/home/santhosh/resumes_folder/keras/extract_summary_and_objective/resume_line_level/model/resume_line_level_iteration_'+str(iteration)+'.h5'\n",
    "    model=load_model(file_path)\n",
    "    # estimate accuracy on whole dataset using loaded weights\n",
    "    scores = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data,verbose=0)\n",
    "    print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "    print(\"Testing Samples\\n\"+\"-\"*50)\n",
    "    print(\"-\"*50)\n",
    "    index=int(np.random.randint(len(input_resumes)/40*0.8))\n",
    "    test_input=\"\"\n",
    "    test_output=\"\"\n",
    "    for i in range(40):\n",
    "        encoded_input_sequence=(encoder_input_data[index]).reshape((1,max_encoder_seq_len,num_encoder_tokens))\n",
    "        #print(encoded_input_sequence.shape,max_encoder_seq_len,num_encoder_tokens,max_encoder_seq_len*num_encoder_tokens)\n",
    "        #print(encoded_input_sequence)\n",
    "        #.reshape((1,max_encoder_seq_len,num_encoder_tokens))\n",
    "        output_sequence=decode_sequence(encoded_input_sequence)\n",
    "        test_input+=input_resumes[index]\n",
    "        test_output+=output_sequence\n",
    "        index+=1\n",
    "    print(\"-\"*50)\n",
    "    print(test_input)\n",
    "    print(\"---OUTPUT-----\")\n",
    "    print(test_output)\n",
    "    print(\" \"*50+\"-\"*50)\n",
    "\n",
    "except:\n",
    "    print('no file exist')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"/home/santhosh/resumes_folder/keras/extract_summary_and_objective/resume_line_level/checkpoints/resume_line_level_checkpoints.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print('Iteration:',iteration+1)\n",
    "        #training\n",
    "        model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2,callbacks=callbacks_list)\n",
    "        #prepare sample_data to test 5 samples:\n",
    "        print(\"-\"*50)\n",
    "        index=int(np.random.randint(len(input_resumes)/40*0.8))\n",
    "        test_input=\"\"\n",
    "        test_output=\"\"\n",
    "        for i in range(40):\n",
    "            encoded_input_sequence=(encoder_input_data[index]).reshape((1,max_encoder_seq_len,num_encoder_tokens))\n",
    "            #print(encoded_input_sequence.shape,max_encoder_seq_len,num_encoder_tokens,max_encoder_seq_len*num_encoder_tokens)\n",
    "            #print(encoded_input_sequence)\n",
    "            #.reshape((1,max_encoder_seq_len,num_encoder_tokens))\n",
    "            output_sequence=decode_sequence(encoded_input_sequence)\n",
    "            test_input+=input_resumes[index]\n",
    "            test_output+=output_sequence\n",
    "            index+=1\n",
    "        print(\"-\"*50)\n",
    "        print(test_input)\n",
    "        print(\"---OUTPUT-----\")\n",
    "        print(test_output)\n",
    "        print(\" \"*50+\"-\"*50)\n",
    "\n",
    "        # Save model\n",
    "        file=open(iteration_file,'a')\n",
    "        file.write('iteration:'+str(iteration+1)+'\\n')\n",
    "        file_path='/home/santhosh/resumes_folder/keras/extract_summary_and_objective/resume_line_level/model/resume_line_level_iteration_'+str(iteration+1)+'.h5'\n",
    "        model.save(file_path)\n",
    "        file.close()\n",
    "        iteration+=1\n",
    "        \n",
    "    except:\n",
    "        file=open(iteration_file,'r')\n",
    "        last_line=file.read().split('\\n')[-2]\n",
    "        print('file_data,',last_line)\n",
    "        iteration=int(last_line.split(':')[1])\n",
    "\n",
    "        #print(iteration)\n",
    "        file.close()\n",
    "        # load weights\n",
    "        print('loading the weights')\n",
    "        file_path='/home/santhosh/resumes_folder/keras/extract_summary_and_objective/resume_line_level/model/resume_line_level_iteration_'+str(iteration)+'.h5'\n",
    "        model=load_model(file_path)\n",
    "        # estimate accuracy on whole dataset using loaded weights\n",
    "        scores = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data,verbose=0)\n",
    "        print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "        print(\"Testing Samples\\n\"+\"-\"*50)\n",
    "        print(\"-\"*50)\n",
    "        index=int(np.random.randint(len(input_resumes)/40*0.8))\n",
    "        test_input=\"\"\n",
    "        test_output=\"\"\n",
    "        for i in range(40):\n",
    "            encoded_input_sequence=(encoder_input_data[index]).reshape((1,max_encoder_seq_len,num_encoder_tokens))\n",
    "            #print(encoded_input_sequence.shape,max_encoder_seq_len,num_encoder_tokens,max_encoder_seq_len*num_encoder_tokens)\n",
    "            #print(encoded_input_sequence)\n",
    "            #.reshape((1,max_encoder_seq_len,num_encoder_tokens))\n",
    "            output_sequence=decode_sequence(encoded_input_sequence)\n",
    "            test_input+=input_resumes[index]\n",
    "            test_output+=output_sequence\n",
    "            index+=1\n",
    "        print(\"-\"*50)\n",
    "        print(test_input)\n",
    "        print(\"---OUTPUT-----\")\n",
    "        print(test_output)\n",
    "        print(\" \"*50+\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
