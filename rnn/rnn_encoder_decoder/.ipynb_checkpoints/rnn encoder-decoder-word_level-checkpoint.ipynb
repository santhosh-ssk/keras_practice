{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model for english to tamil transilation using rnn encoder and decoder ( at word level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset is downloaded from http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/envs/pydl/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input,LSTM,Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "batch_size=128\n",
    "epochs=25\n",
    "latent_dim=256\n",
    "data_path=\"/home/santhosh/keras/rnn/rnn_encoder_decoder/data/tam.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_words=set()\n",
    "target_words=set()\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines=f.read().split('\\n')\n",
    "for line in lines:\n",
    "    line=line.split('\\t')\n",
    "    if(len(line)!=2):\n",
    "        continue\n",
    "    input_text,target_text=line\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text='<SOL> '+target_text+' <EOL>'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for word in input_text.split():\n",
    "        if word not in input_words:\n",
    "            input_words.add(word)\n",
    "    for word in target_text.split():\n",
    "        if word not in target_words:\n",
    "            target_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 180\n",
      "number of unique input token: 424\n",
      "number of unique output token: 497\n",
      "Max Sequence length for inputs: 19\n",
      "Max Sequence length for outputs: 13\n"
     ]
    }
   ],
   "source": [
    "input_words=sorted(list(input_words))\n",
    "targer_words=sorted(list(target_words))\n",
    "num_encoder_tokens=len(input_words)\n",
    "num_decoder_tokens=len(target_words)\n",
    "max_encoder_seq_len=max([len(text.split()) for text in input_texts])\n",
    "max_decoder_seq_len=max([len(text.split()) for text in target_texts])\n",
    "\n",
    "print('Number of samples:',len(input_texts))\n",
    "print('number of unique input token:',num_encoder_tokens)\n",
    "print('number of unique output token:',num_decoder_tokens)\n",
    "print('Max Sequence length for inputs:',max_encoder_seq_len)\n",
    "print('Max Sequence length for outputs:',max_decoder_seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token2index=dict([(word,i) for i,word in enumerate(input_words)])\n",
    "target_token2index=dict([(word,i) for i,word in enumerate(target_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defing encoder_input,decoder_input and decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros((len(input_texts),max_encoder_seq_len,num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data=np.zeros((len(input_texts),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data=np.zeros((len(input_texts),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i,t,input_token2index[word]]=1\n",
    "        \n",
    "    for t,word in enumerate(target_text.split()):\n",
    "        decoder_input_data[i,t,target_token2index[word]]=1\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        if t>0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i,t-1,target_token2index[word]]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an input sequence and process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Set up the decoder, using encoder_states as initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model that will turn\n",
    "## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_word_index = dict(\n",
    "    (i, word) for word, i in input_token2index.items())\n",
    "reverse_target_word_index = dict(\n",
    "    (i, word) for word, i in target_token2index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token2index['<SOL>']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == '<EOL>' or\n",
    "           len(decoded_sentence.split()) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    if('<EOL>' in decoded_sentence):\n",
    "        decoded_sentence=\" \".join(decoded_sentence.split()[:-1])\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the weights\n",
      "acc: 30.09%\n",
      "\n",
      "\n",
      "Testing Samples\n",
      "--------------------------------------------------\n",
      "It's a piece of cake. இது ஒரு கேக்கின் துண்டு\n",
      "Come and see me right now. உடனே வந்து என்னைப் பார்க்கவும்\n",
      "When did you come to Japan? அவன் ஓட விருப்பப் படுகிறான்\n",
      "Don't drink and drive. குடித்துவிட்டு வண்டி ஓட்டாதே\n",
      "Do you have a lot of pens? உன்னிடம் நிறைய பேனாக்கள் இருக்கின்றனவா?\n",
      "--------------------------------------------------\n",
      "file_data, iteration:65\n",
      "Iteration: 66\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/25\n",
      "144/144 [==============================] - 4s 25ms/step - loss: 2.2977e-05 - acc: 0.3542 - val_loss: 6.0619 - val_acc: 0.0876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/envs/pydl/lib/python3.5/site-packages/keras/engine/network.py:888: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1_2/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_1_2/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 2.2415e-05 - acc: 0.3542 - val_loss: 6.0665 - val_acc: 0.0876\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 2.1443e-05 - acc: 0.3542 - val_loss: 6.0616 - val_acc: 0.0876\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 2.1225e-05 - acc: 0.3542 - val_loss: 6.0646 - val_acc: 0.0876\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 2.0248e-05 - acc: 0.3542 - val_loss: 6.0691 - val_acc: 0.0876\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.9732e-05 - acc: 0.3542 - val_loss: 6.0735 - val_acc: 0.0876\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.9039e-05 - acc: 0.3542 - val_loss: 6.0656 - val_acc: 0.0876\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.8515e-05 - acc: 0.3542 - val_loss: 6.0769 - val_acc: 0.0876\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.8575e-05 - acc: 0.3542 - val_loss: 6.0580 - val_acc: 0.0876\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.8624e-05 - acc: 0.3542 - val_loss: 6.0670 - val_acc: 0.0876\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.7724e-05 - acc: 0.3542 - val_loss: 6.0679 - val_acc: 0.0876\n",
      "Epoch 12/25\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 0.0236 - acc: 0.3499 - val_loss: 6.0045 - val_acc: 0.0876\n",
      "Epoch 13/25\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 0.2271 - acc: 0.3157 - val_loss: 6.0761 - val_acc: 0.0855\n",
      "Epoch 14/25\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.0709 - acc: 0.3371 - val_loss: 6.0420 - val_acc: 0.0876\n",
      "Epoch 15/25\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 0.0313 - acc: 0.3483 - val_loss: 6.0433 - val_acc: 0.0876\n",
      "Epoch 16/25\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 0.0082 - acc: 0.3526 - val_loss: 6.0411 - val_acc: 0.0876\n",
      "Epoch 17/25\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 0.0038 - acc: 0.3536 - val_loss: 6.0363 - val_acc: 0.0876\n",
      "Epoch 18/25\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 0.0014 - acc: 0.3536 - val_loss: 6.0337 - val_acc: 0.0876\n",
      "Epoch 19/25\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 7.8786e-04 - acc: 0.3542 - val_loss: 6.0306 - val_acc: 0.0876\n",
      "Epoch 20/25\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 5.9335e-04 - acc: 0.3542 - val_loss: 6.0342 - val_acc: 0.0876\n",
      "Epoch 21/25\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 4.2764e-04 - acc: 0.3542 - val_loss: 6.0325 - val_acc: 0.0876\n",
      "Epoch 22/25\n",
      "128/144 [=========================>....] - ETA: 0s - loss: 3.2445e-04 - acc: 0.3558"
     ]
    }
   ],
   "source": [
    "\n",
    "iteration=0\n",
    "\n",
    "# load weights\n",
    "print('loading the weights')\n",
    "model=load_model('word_level.h5')\n",
    "\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data,verbose=0)\n",
    "print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"Testing Samples\\n\"+\"-\"*50)\n",
    "for i in range(5):\n",
    "    index=np.random.randint(len(input_texts))\n",
    "    encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "    output_sequence=decode_sequence(encoded_input_sequence)\n",
    "    print(input_texts[index],output_sequence)\n",
    "print(\"-\"*50)\n",
    "\n",
    "iteration_file=\"/home/santhosh/keras/rnn/rnn_encoder_decoder/iteration_word_level.txt\"\n",
    "try:\n",
    "    file=open(iteration_file,'r')\n",
    "    last_line=file.read().split('\\n')[-2]\n",
    "    print('file_data,',last_line)\n",
    "    iteration=int(last_line.split(':')[1])\n",
    "    #print(iteration)\n",
    "    file.close()\n",
    "    \n",
    "except:\n",
    "    print('no file exist')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights_best_word_level.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "while True:\n",
    "    print('Iteration:',iteration+1)\n",
    "    #training\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,callbacks=callbacks_list)\n",
    "    #prepare sample_data to test 5 samples:\n",
    "    print(\"-\"*50)\n",
    "    for i in range(5):\n",
    "        index=np.random.randint(len(input_texts))\n",
    "        encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "        output_sequence=decode_sequence(encoded_input_sequence)\n",
    "        print(input_texts[index],output_sequence)\n",
    "    print(\"-\"*50)\n",
    "    # Save model\n",
    "    file=open(iteration_file,'a')\n",
    "    file.write('iteration:'+str(iteration+1)+'\\n')\n",
    "    file.close()\n",
    "    iteration+=1\n",
    "    model.save('word_level.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
