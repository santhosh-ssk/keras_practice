{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence model for english to tamil transilation using rnn encoder and decoder ( at character level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset is downloaded from http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/envs/pydl/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input,LSTM,Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "batch_size=64\n",
    "epochs=25\n",
    "latent_dim=256\n",
    "data_path=\"/home/santhosh/keras/rnn/rnn_encoder_decoder/data/tam.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines=f.read().split('\\n')\n",
    "for line in lines:\n",
    "    line=line.split('\\t')\n",
    "    if(len(line)!=2):\n",
    "        continue\n",
    "    input_text,target_text=line\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text='\\t'+target_text+'\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 180\n",
      "number of unique input token: 53\n",
      "number of unique output token: 54\n",
      "Max Sequence length for inputs: 94\n",
      "Max Sequence length for outputs: 111\n"
     ]
    }
   ],
   "source": [
    "input_characters=sorted(list(input_characters))\n",
    "targer_characters=sorted(list(target_characters))\n",
    "num_encoder_tokens=len(input_characters)\n",
    "num_decoder_tokens=len(target_characters)\n",
    "max_encoder_seq_len=max([len(text) for text in input_texts])\n",
    "max_decoder_seq_len=max([len(text) for text in target_texts])\n",
    "\n",
    "print('Number of samples:',len(input_texts))\n",
    "print('number of unique input token:',num_encoder_tokens)\n",
    "print('number of unique output token:',num_decoder_tokens)\n",
    "print('Max Sequence length for inputs:',max_encoder_seq_len)\n",
    "print('Max Sequence length for outputs:',max_decoder_seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining token2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token2index=dict([(char,i) for i,char in enumerate(input_characters)])\n",
    "target_token2index=dict([(char,i) for i,char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defing encoder_input,decoder_input and decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data=np.zeros((len(input_texts),max_encoder_seq_len,num_encoder_tokens),dtype='float32')\n",
    "decoder_input_data=np.zeros((len(input_texts),max_decoder_seq_len,num_decoder_tokens),dtype='float32')\n",
    "decoder_target_data=np.zeros((len(input_texts),max_decoder_seq_len,num_decoder_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n",
    "    for t,char in enumerate(input_text):\n",
    "        encoder_input_data[i,t,input_token2index[char]]=1\n",
    "    for t,char in enumerate(target_text):\n",
    "        decoder_input_data[i,t,target_token2index[char]]=1\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        if t>0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i,t-1,target_token2index[char]]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an input sequence and process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Set up the decoder, using encoder_states as initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model that will turn\n",
    "## `encoder_input_data` & `decoder_input_data` into `decoder_target_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token2index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token2index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token2index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file exist\n",
      "Iteration: 1\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.7078 - acc: 0.8024 - val_loss: 1.2087 - val_acc: 0.6829\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/envs/pydl/lib/python3.5/site-packages/keras/engine/network.py:888: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3_1/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_3_1/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 7s 47ms/step - loss: 0.7040 - acc: 0.4950 - val_loss: 1.2146 - val_acc: 0.6762\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 7s 49ms/step - loss: 0.6941 - acc: 0.8037 - val_loss: 1.1943 - val_acc: 0.6897\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.6882 - acc: 0.8108 - val_loss: 1.1677 - val_acc: 0.6794\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.6900 - acc: 0.7140 - val_loss: 1.1791 - val_acc: 0.6794\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.6767 - acc: 0.4929 - val_loss: 1.1689 - val_acc: 0.6847\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.6741 - acc: 0.8072 - val_loss: 1.1682 - val_acc: 0.6899\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.6658 - acc: 0.7367 - val_loss: 1.1683 - val_acc: 0.6839\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.6573 - acc: 0.5067 - val_loss: 1.1774 - val_acc: 0.1094\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.6505 - acc: 0.5013 - val_loss: 1.1905 - val_acc: 0.6872\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.6577 - acc: 0.8031 - val_loss: 1.1501 - val_acc: 0.1051\n",
      "Epoch 12/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.6430 - acc: 0.4931 - val_loss: 1.1344 - val_acc: 0.1374\n",
      "Epoch 13/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.6349 - acc: 0.5156 - val_loss: 1.1327 - val_acc: 0.6814\n",
      "Epoch 14/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.6291 - acc: 0.8148 - val_loss: 1.0993 - val_acc: 0.1627\n",
      "Epoch 15/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.6187 - acc: 0.5313 - val_loss: 1.1098 - val_acc: 0.6889\n",
      "Epoch 16/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.6144 - acc: 0.8158 - val_loss: 1.0875 - val_acc: 0.6942\n",
      "Epoch 17/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.6247 - acc: 0.7711 - val_loss: 1.1113 - val_acc: 0.6794\n",
      "Epoch 18/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.6135 - acc: 0.4986 - val_loss: 1.0965 - val_acc: 0.6752\n",
      "Epoch 19/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.5988 - acc: 0.8176 - val_loss: 1.0717 - val_acc: 0.7022\n",
      "Epoch 20/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.5930 - acc: 0.7446 - val_loss: 1.0998 - val_acc: 0.6972\n",
      "Epoch 21/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.5905 - acc: 0.5337 - val_loss: 1.0740 - val_acc: 0.7005\n",
      "Epoch 22/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.5894 - acc: 0.5094 - val_loss: 1.0769 - val_acc: 0.1319\n",
      "Epoch 23/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.5812 - acc: 0.4364 - val_loss: 1.0951 - val_acc: 0.1201\n",
      "Epoch 24/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.5852 - acc: 0.5125 - val_loss: 1.0456 - val_acc: 0.7032\n",
      "Epoch 25/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.5707 - acc: 0.8255 - val_loss: 1.0911 - val_acc: 0.6321\n",
      "--------------------------------------------------\n",
      "I am engaged to her. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "Come and help us. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "He is fond of swimming. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "He is afraid of death. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "She glanced through the magazine. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "--------------------------------------------------\n",
      "Iteration: 2\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.5773 - acc: 0.8037 - val_loss: 1.0498 - val_acc: 0.7080\n",
      "Epoch 2/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.5586 - acc: 0.8292 - val_loss: 1.0424 - val_acc: 0.1404\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 6s 43ms/step - loss: 0.5962 - acc: 0.5035 - val_loss: 1.0447 - val_acc: 0.1346\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.5754 - acc: 0.4933 - val_loss: 1.0180 - val_acc: 0.7155\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 7s 49ms/step - loss: 0.5652 - acc: 0.8209 - val_loss: 1.0355 - val_acc: 0.1296\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.5641 - acc: 0.4286 - val_loss: 1.0388 - val_acc: 0.2535\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.5563 - acc: 0.5541 - val_loss: 1.0178 - val_acc: 0.1391\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.5468 - acc: 0.5173 - val_loss: 1.0194 - val_acc: 0.6694\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.5357 - acc: 0.8181 - val_loss: 1.0593 - val_acc: 0.6299\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.5392 - acc: 0.7810 - val_loss: 0.9989 - val_acc: 0.6844\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.5267 - acc: 0.8186 - val_loss: 1.0099 - val_acc: 0.1476\n",
      "Epoch 12/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.5255 - acc: 0.5176 - val_loss: 1.0029 - val_acc: 0.1459\n",
      "Epoch 13/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.5210 - acc: 0.4763 - val_loss: 0.9763 - val_acc: 0.6491\n",
      "Epoch 14/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.5135 - acc: 0.7381 - val_loss: 1.0022 - val_acc: 0.6887\n",
      "Epoch 15/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.5127 - acc: 0.5386 - val_loss: 1.0069 - val_acc: 0.2310\n",
      "Epoch 16/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.5076 - acc: 0.5091 - val_loss: 1.0068 - val_acc: 0.7145\n",
      "Epoch 17/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.5101 - acc: 0.5037 - val_loss: 0.9775 - val_acc: 0.1502\n",
      "Epoch 18/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.4955 - acc: 0.4430 - val_loss: 0.9721 - val_acc: 0.7085\n",
      "Epoch 19/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4980 - acc: 0.4997 - val_loss: 0.9627 - val_acc: 0.2355\n",
      "Epoch 20/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4870 - acc: 0.2506 - val_loss: 0.9763 - val_acc: 0.6764\n",
      "Epoch 21/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4905 - acc: 0.7721 - val_loss: 0.9916 - val_acc: 0.1619\n",
      "Epoch 22/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4854 - acc: 0.5166 - val_loss: 0.9965 - val_acc: 0.1389\n",
      "Epoch 23/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4865 - acc: 0.4476 - val_loss: 0.9850 - val_acc: 0.1449\n",
      "Epoch 24/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.4806 - acc: 0.5077 - val_loss: 0.9772 - val_acc: 0.1727\n",
      "Epoch 25/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4671 - acc: 0.1848 - val_loss: 0.9538 - val_acc: 0.1647\n",
      "--------------------------------------------------\n",
      "I have to dress up. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "Which of them is your brother? ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "Keep in touch! ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "I expect him to come. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "She has never been in a car driven by him. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "--------------------------------------------------\n",
      "Iteration: 3\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4643 - acc: 0.4535 - val_loss: 0.9640 - val_acc: 0.5991\n",
      "Epoch 2/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4702 - acc: 0.6846 - val_loss: 0.9920 - val_acc: 0.2080\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4767 - acc: 0.2418 - val_loss: 0.9525 - val_acc: 0.2082\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4618 - acc: 0.3507 - val_loss: 0.9576 - val_acc: 0.4162\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.4592 - acc: 0.3141 - val_loss: 0.9643 - val_acc: 0.1406\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.4542 - acc: 0.1530 - val_loss: 0.9634 - val_acc: 0.1436\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4511 - acc: 0.4313 - val_loss: 0.9357 - val_acc: 0.1727\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4566 - acc: 0.4587 - val_loss: 0.9360 - val_acc: 0.5115\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4467 - acc: 0.4260 - val_loss: 0.9460 - val_acc: 0.2990\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4405 - acc: 0.4833 - val_loss: 0.9586 - val_acc: 0.1446\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 7s 50ms/step - loss: 0.4573 - acc: 0.4112 - val_loss: 0.9430 - val_acc: 0.1509\n",
      "Epoch 12/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4467 - acc: 0.1413 - val_loss: 0.9428 - val_acc: 0.2422\n",
      "Epoch 13/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4344 - acc: 0.2035 - val_loss: 0.9220 - val_acc: 0.3624\n",
      "Epoch 14/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4317 - acc: 0.3212 - val_loss: 0.9674 - val_acc: 0.1451\n",
      "Epoch 15/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4272 - acc: 0.2130 - val_loss: 0.9397 - val_acc: 0.1737\n",
      "Epoch 16/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4262 - acc: 0.2126 - val_loss: 0.9284 - val_acc: 0.3554\n",
      "Epoch 17/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.4220 - acc: 0.3424 - val_loss: 0.9332 - val_acc: 0.4402\n",
      "Epoch 18/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4174 - acc: 0.3975 - val_loss: 0.9450 - val_acc: 0.1429\n",
      "Epoch 19/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4151 - acc: 0.1489 - val_loss: 0.9153 - val_acc: 0.1552\n",
      "Epoch 20/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.4153 - acc: 0.2167 - val_loss: 0.9264 - val_acc: 0.1574\n",
      "Epoch 21/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4038 - acc: 0.1797 - val_loss: 0.9422 - val_acc: 0.1709\n",
      "Epoch 22/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.4047 - acc: 0.1650 - val_loss: 0.9503 - val_acc: 0.2975\n",
      "Epoch 23/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.4016 - acc: 0.2526 - val_loss: 0.9556 - val_acc: 0.1522\n",
      "Epoch 24/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3999 - acc: 0.3725 - val_loss: 0.9345 - val_acc: 0.1512\n",
      "Epoch 25/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3945 - acc: 0.1644 - val_loss: 0.9552 - val_acc: 0.1499\n",
      "--------------------------------------------------\n",
      "When does it begin? ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "A square has four sides. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "You keep out of this. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "He went in place of me. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "I have to dress up. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "--------------------------------------------------\n",
      "Iteration: 4\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.3930 - acc: 0.1639 - val_loss: 0.9200 - val_acc: 0.1584\n",
      "Epoch 2/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.3867 - acc: 0.2041 - val_loss: 0.9486 - val_acc: 0.1834\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 7s 49ms/step - loss: 0.3927 - acc: 0.2427 - val_loss: 0.9479 - val_acc: 0.1944\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3828 - acc: 0.1893 - val_loss: 0.9453 - val_acc: 0.1587\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3804 - acc: 0.1661 - val_loss: 0.9351 - val_acc: 0.1779\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3746 - acc: 0.1790 - val_loss: 0.9367 - val_acc: 0.1814\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.3696 - acc: 0.2245 - val_loss: 0.9518 - val_acc: 0.1829\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.3701 - acc: 0.1883 - val_loss: 0.9470 - val_acc: 0.1644\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.3730 - acc: 0.1715 - val_loss: 0.9521 - val_acc: 0.1564\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3669 - acc: 0.2039 - val_loss: 0.9278 - val_acc: 0.3741\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3659 - acc: 0.2947 - val_loss: 0.9572 - val_acc: 0.2012\n",
      "Epoch 12/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.3654 - acc: 0.2040 - val_loss: 0.9270 - val_acc: 0.1934\n",
      "Epoch 13/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.3610 - acc: 0.1969 - val_loss: 0.9293 - val_acc: 0.2265\n",
      "Epoch 14/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.3535 - acc: 0.2133 - val_loss: 0.9678 - val_acc: 0.1662\n",
      "Epoch 15/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.3537 - acc: 0.2093 - val_loss: 0.9484 - val_acc: 0.1969\n",
      "Epoch 16/25\n",
      "144/144 [==============================] - 6s 43ms/step - loss: 0.3539 - acc: 0.2302 - val_loss: 0.9234 - val_acc: 0.1604\n",
      "Epoch 17/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3545 - acc: 0.2415 - val_loss: 0.9342 - val_acc: 0.2110\n",
      "Epoch 18/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3477 - acc: 0.2250 - val_loss: 0.9722 - val_acc: 0.1542\n",
      "Epoch 19/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3603 - acc: 0.2058 - val_loss: 0.9364 - val_acc: 0.1824\n",
      "Epoch 20/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3433 - acc: 0.2171 - val_loss: 0.9524 - val_acc: 0.2437\n",
      "Epoch 21/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.3373 - acc: 0.2614 - val_loss: 0.9214 - val_acc: 0.3063\n",
      "Epoch 22/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3464 - acc: 0.3261 - val_loss: 0.9408 - val_acc: 0.1602\n",
      "Epoch 23/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3321 - acc: 0.2589 - val_loss: 0.9438 - val_acc: 0.2195\n",
      "Epoch 24/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3241 - acc: 0.2598 - val_loss: 0.9556 - val_acc: 0.1607\n",
      "Epoch 25/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3257 - acc: 0.2289 - val_loss: 0.9538 - val_acc: 0.2808\n",
      "--------------------------------------------------\n",
      "He let go of the rope. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "I don't think people use that word anymore. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "She is not afraid of anything. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "Give it to her. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "This apple is sweet. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "--------------------------------------------------\n",
      "Iteration: 5\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3339 - acc: 0.2554 - val_loss: 0.9515 - val_acc: 0.2718\n",
      "Epoch 2/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3214 - acc: 0.3133 - val_loss: 0.9618 - val_acc: 0.2457\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 7s 45ms/step - loss: 0.3221 - acc: 0.2589 - val_loss: 0.9685 - val_acc: 0.2142\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3245 - acc: 0.2279 - val_loss: 0.9742 - val_acc: 0.2307\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3210 - acc: 0.2401 - val_loss: 0.9454 - val_acc: 0.2220\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3123 - acc: 0.2489 - val_loss: 0.9640 - val_acc: 0.1652\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.3107 - acc: 0.2282 - val_loss: 0.9656 - val_acc: 0.1922\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.3003 - acc: 0.2342 - val_loss: 0.9489 - val_acc: 0.2968\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.2995 - acc: 0.3074 - val_loss: 0.9732 - val_acc: 0.1989\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.2984 - acc: 0.2833 - val_loss: 0.9480 - val_acc: 0.2312\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.2915 - acc: 0.2667 - val_loss: 0.9761 - val_acc: 0.1707\n",
      "Epoch 12/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2903 - acc: 0.2434 - val_loss: 1.0083 - val_acc: 0.2227\n",
      "Epoch 13/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2965 - acc: 0.2575 - val_loss: 0.9773 - val_acc: 0.1647\n",
      "Epoch 14/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2926 - acc: 0.2396 - val_loss: 0.9717 - val_acc: 0.1932\n",
      "Epoch 15/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2918 - acc: 0.2377 - val_loss: 0.9834 - val_acc: 0.2215\n",
      "Epoch 16/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.2773 - acc: 0.2501 - val_loss: 0.9681 - val_acc: 0.1954\n",
      "Epoch 17/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2779 - acc: 0.2526 - val_loss: 0.9727 - val_acc: 0.1694\n",
      "Epoch 18/25\n",
      "144/144 [==============================] - 7s 50ms/step - loss: 0.2718 - acc: 0.2313 - val_loss: 0.9886 - val_acc: 0.2040\n",
      "Epoch 19/25\n",
      "144/144 [==============================] - 7s 50ms/step - loss: 0.2789 - acc: 0.2807 - val_loss: 0.9739 - val_acc: 0.2505\n",
      "Epoch 20/25\n",
      "144/144 [==============================] - 8s 53ms/step - loss: 0.2704 - acc: 0.3017 - val_loss: 1.0095 - val_acc: 0.1839\n",
      "Epoch 21/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2720 - acc: 0.3141 - val_loss: 1.0006 - val_acc: 0.3131\n",
      "Epoch 22/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.2711 - acc: 0.2850 - val_loss: 0.9747 - val_acc: 0.2360\n",
      "Epoch 23/25\n",
      "144/144 [==============================] - 6s 45ms/step - loss: 0.2578 - acc: 0.2995 - val_loss: 0.9973 - val_acc: 0.2487\n",
      "Epoch 24/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.2807 - acc: 0.2753 - val_loss: 0.9895 - val_acc: 0.1654\n",
      "Epoch 25/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.2557 - acc: 0.2614 - val_loss: 1.0008 - val_acc: 0.2452\n",
      "--------------------------------------------------\n",
      "Come and see me. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "I know every inch of the town. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "She sat next to me. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "I don't think people use that word anymore. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "I am tired of my work. ிளல!)ு)??ழஜஜஜஜஜCஜஜC(உஉ. )C2CஇனோCோCஇCஇ னனல!கன00ல!குன00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக00லக00்்\tிளளூூலக00லக0\n",
      "--------------------------------------------------\n",
      "Iteration: 6\n",
      "Train on 144 samples, validate on 36 samples\n",
      "Epoch 1/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.2526 - acc: 0.3038 - val_loss: 1.0224 - val_acc: 0.3086\n",
      "Epoch 2/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.2585 - acc: 0.3164 - val_loss: 0.9793 - val_acc: 0.2200\n",
      "Epoch 3/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.2507 - acc: 0.3097 - val_loss: 1.0058 - val_acc: 0.1814\n",
      "Epoch 4/25\n",
      "144/144 [==============================] - 6s 43ms/step - loss: 0.2443 - acc: 0.2717 - val_loss: 1.0181 - val_acc: 0.1999\n",
      "Epoch 5/25\n",
      "144/144 [==============================] - 6s 44ms/step - loss: 0.2448 - acc: 0.2892 - val_loss: 1.0291 - val_acc: 0.2095\n",
      "Epoch 6/25\n",
      "144/144 [==============================] - 7s 49ms/step - loss: 0.2336 - acc: 0.2737 - val_loss: 1.0199 - val_acc: 0.2285\n",
      "Epoch 7/25\n",
      "144/144 [==============================] - 7s 46ms/step - loss: 0.2337 - acc: 0.2787 - val_loss: 1.0298 - val_acc: 0.1667\n",
      "Epoch 8/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2336 - acc: 0.2648 - val_loss: 1.0227 - val_acc: 0.2067\n",
      "Epoch 9/25\n",
      "144/144 [==============================] - 7s 48ms/step - loss: 0.2301 - acc: 0.2690 - val_loss: 1.0260 - val_acc: 0.2252\n",
      "Epoch 10/25\n",
      "144/144 [==============================] - 7s 49ms/step - loss: 0.2320 - acc: 0.2929 - val_loss: 1.0240 - val_acc: 0.2295\n",
      "Epoch 11/25\n",
      "144/144 [==============================] - 7s 47ms/step - loss: 0.2187 - acc: 0.2827 - val_loss: 1.0663 - val_acc: 0.1919\n",
      "Epoch 12/25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iteration=0\n",
    "\n",
    "# load weights\n",
    "print('loading the weights')\n",
    "model=load_model('char_level.h5')\n",
    "\n",
    "# estimate accuracy on whole dataset using loaded weights\n",
    "scores = model.evaluate([encoder_input_data, decoder_input_data], decoder_target_data,verbose=0)\n",
    "print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"Testing Samples\\n\"+\"-\"*50)\n",
    "for i in range(5):\n",
    "    index=np.random.randint(len(input_texts))\n",
    "    encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "    output_sequence=decode_sequence(encoded_input_sequence)\n",
    "    print(input_texts[index],output_sequence)\n",
    "print(\"-\"*50)\n",
    "\n",
    "iteration_file=\"/home/santhosh/keras/rnn/rnn_encoder_decoder/iteration_char_level.txt\"\n",
    "try:\n",
    "    file=open(iteration_file,'r')\n",
    "    last_line=file.read().split('\\n')[-2]\n",
    "    print('file_data,',last_line)\n",
    "    iteration=int(last_line.split(':')[1])\n",
    "    #print(iteration)\n",
    "    file.close()\n",
    "    \n",
    "except:\n",
    "    print('no file exist')\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights_best_char_level.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=0, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "while True:\n",
    "    print('Iteration:',iteration+1)\n",
    "    #training\n",
    "    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,callbacks=callbacks_list)\n",
    "    #prepare sample_data to test 5 samples:\n",
    "    print(\"-\"*50)\n",
    "    for i in range(5):\n",
    "        index=np.random.randint(len(input_texts))\n",
    "        encoded_input_sequence=encoder_input_data[index: index + 1]\n",
    "        output_sequence=decode_sequence(encoded_input_sequence)\n",
    "        print(input_texts[index],output_sequence)\n",
    "    print(\"-\"*50)\n",
    "    # Save model\n",
    "    file=open(iteration_file,'a')\n",
    "    file.write('iteration:'+str(iteration+1)+'\\n')\n",
    "    file.close()\n",
    "    iteration+=1\n",
    "    model.save('char_level.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
