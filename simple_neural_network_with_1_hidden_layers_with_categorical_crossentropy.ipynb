{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple neural network program with one hidden node,using categorical_crossentropy loss function, relu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)  128 neurons,batch_size=128,epochs=20,Activation=relu and testScore: 0.9316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 54us/step - loss: 1.2819 - acc: 0.6906 - val_loss: 0.7190 - val_acc: 0.8435\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.6254 - acc: 0.8497 - val_loss: 0.4984 - val_acc: 0.8783\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 52us/step - loss: 0.4891 - acc: 0.8756 - val_loss: 0.4203 - val_acc: 0.8931\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.4279 - acc: 0.8866 - val_loss: 0.3790 - val_acc: 0.9007\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.3919 - acc: 0.8939 - val_loss: 0.3528 - val_acc: 0.9050\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3672 - acc: 0.8995 - val_loss: 0.3342 - val_acc: 0.9087\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.3489 - acc: 0.9038 - val_loss: 0.3197 - val_acc: 0.9122\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.3343 - acc: 0.9073 - val_loss: 0.3088 - val_acc: 0.9157\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.3222 - acc: 0.9097 - val_loss: 0.2984 - val_acc: 0.9172\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3116 - acc: 0.9129 - val_loss: 0.2901 - val_acc: 0.9190\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.3023 - acc: 0.9155 - val_loss: 0.2824 - val_acc: 0.9207\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2939 - acc: 0.9173 - val_loss: 0.2754 - val_acc: 0.9220\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2863 - acc: 0.9194 - val_loss: 0.2696 - val_acc: 0.9239\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.2792 - acc: 0.9212 - val_loss: 0.2637 - val_acc: 0.9256\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.2726 - acc: 0.9232 - val_loss: 0.2583 - val_acc: 0.9264\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2664 - acc: 0.9250 - val_loss: 0.2537 - val_acc: 0.9279\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.2607 - acc: 0.9266 - val_loss: 0.2485 - val_acc: 0.9289\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.2552 - acc: 0.9278 - val_loss: 0.2439 - val_acc: 0.9315\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2501 - acc: 0.9295 - val_loss: 0.2400 - val_acc: 0.9322\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.2452 - acc: 0.9314 - val_loss: 0.2359 - val_acc: 0.9338\n",
      "10000/10000 [==============================] - 0s 46us/step\n",
      "Test score: 0.2344033989906311\n",
      "Test accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=128\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=128,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)  128 neurons,batch_size=128,epochs=20,Activation=sigmoid and testScore: 0.8957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_42 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 2.1145 - acc: 0.4384 - val_loss: 1.8719 - val_acc: 0.6661\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 1.6870 - acc: 0.7044 - val_loss: 1.4901 - val_acc: 0.7684\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 1.3565 - acc: 0.7696 - val_loss: 1.1987 - val_acc: 0.8060\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 51us/step - loss: 1.1167 - acc: 0.8006 - val_loss: 0.9947 - val_acc: 0.8314\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.9513 - acc: 0.8194 - val_loss: 0.8555 - val_acc: 0.8412\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.8358 - acc: 0.8320 - val_loss: 0.7568 - val_acc: 0.8553\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 58us/step - loss: 0.7524 - acc: 0.8431 - val_loss: 0.6843 - val_acc: 0.8593\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.6898 - acc: 0.8509 - val_loss: 0.6300 - val_acc: 0.8656\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.6414 - acc: 0.8564 - val_loss: 0.5871 - val_acc: 0.8709\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 57us/step - loss: 0.6028 - acc: 0.8619 - val_loss: 0.5527 - val_acc: 0.8737\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.5714 - acc: 0.8652 - val_loss: 0.5248 - val_acc: 0.8778\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.5453 - acc: 0.8686 - val_loss: 0.5015 - val_acc: 0.8806\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 47us/step - loss: 0.5233 - acc: 0.8724 - val_loss: 0.4820 - val_acc: 0.8844\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.5044 - acc: 0.8751 - val_loss: 0.4653 - val_acc: 0.8858\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.4882 - acc: 0.8771 - val_loss: 0.4508 - val_acc: 0.8890\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.4740 - acc: 0.8796 - val_loss: 0.4379 - val_acc: 0.8913\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.4615 - acc: 0.8813 - val_loss: 0.4269 - val_acc: 0.8918\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 50us/step - loss: 0.4503 - acc: 0.8832 - val_loss: 0.4170 - val_acc: 0.8937\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 48us/step - loss: 0.4404 - acc: 0.8846 - val_loss: 0.4082 - val_acc: 0.8958\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 49us/step - loss: 0.4314 - acc: 0.8865 - val_loss: 0.4001 - val_acc: 0.8965\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "Test score: 0.4026079479932785\n",
      "Test accuracy: 0.8957\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=128\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=128,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)  30 neurons,batch_size=128,epochs=20,Activation=relu and testScore: 0.9262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 30)                23550     \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 23,860\n",
      "Trainable params: 23,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 3s 53us/step - loss: 1.5567 - acc: 0.5560 - val_loss: 0.9458 - val_acc: 0.7934\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.7539 - acc: 0.8230 - val_loss: 0.5772 - val_acc: 0.8603\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.5475 - acc: 0.8608 - val_loss: 0.4660 - val_acc: 0.8815\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.4669 - acc: 0.8752 - val_loss: 0.4127 - val_acc: 0.8918\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.4230 - acc: 0.8853 - val_loss: 0.3813 - val_acc: 0.8964\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3945 - acc: 0.8907 - val_loss: 0.3597 - val_acc: 0.9004\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3741 - acc: 0.8956 - val_loss: 0.3447 - val_acc: 0.9049\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.3587 - acc: 0.8999 - val_loss: 0.3323 - val_acc: 0.9064\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.3462 - acc: 0.9028 - val_loss: 0.3224 - val_acc: 0.9087\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 43us/step - loss: 0.3361 - acc: 0.9049 - val_loss: 0.3143 - val_acc: 0.9117\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.3272 - acc: 0.9074 - val_loss: 0.3071 - val_acc: 0.9131\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.3194 - acc: 0.9096 - val_loss: 0.3008 - val_acc: 0.9156\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3124 - acc: 0.9115 - val_loss: 0.2943 - val_acc: 0.9173\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3059 - acc: 0.9132 - val_loss: 0.2907 - val_acc: 0.9176\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3001 - acc: 0.9154 - val_loss: 0.2843 - val_acc: 0.9195\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2943 - acc: 0.9167 - val_loss: 0.2804 - val_acc: 0.9205\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.2892 - acc: 0.9183 - val_loss: 0.2761 - val_acc: 0.9225\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.2843 - acc: 0.9199 - val_loss: 0.2717 - val_acc: 0.9226\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.2797 - acc: 0.9214 - val_loss: 0.2682 - val_acc: 0.9237\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.2755 - acc: 0.9229 - val_loss: 0.2650 - val_acc: 0.9235\n",
      "10000/10000 [==============================] - 0s 42us/step\n",
      "Test score: 0.26708932144045827\n",
      "Test accuracy: 0.9262\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=30\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=128,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)  256 neurons,batch_size=128,epochs=20,Activation=relu and testScore: 0.9368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 203,530\n",
      "Trainable params: 203,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 1.2644 - acc: 0.6986 - val_loss: 0.7090 - val_acc: 0.8481\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.6109 - acc: 0.8561 - val_loss: 0.4873 - val_acc: 0.8788\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.4777 - acc: 0.8782 - val_loss: 0.4120 - val_acc: 0.8943\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.4191 - acc: 0.8888 - val_loss: 0.3722 - val_acc: 0.9019\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.3846 - acc: 0.8960 - val_loss: 0.3474 - val_acc: 0.9075\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.3610 - acc: 0.9005 - val_loss: 0.3290 - val_acc: 0.9113\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3432 - acc: 0.9052 - val_loss: 0.3151 - val_acc: 0.9151\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3289 - acc: 0.9087 - val_loss: 0.3043 - val_acc: 0.9168\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.3169 - acc: 0.9120 - val_loss: 0.2946 - val_acc: 0.9183\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.3065 - acc: 0.9143 - val_loss: 0.2858 - val_acc: 0.9213\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2974 - acc: 0.9168 - val_loss: 0.2781 - val_acc: 0.9235\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.2890 - acc: 0.9192 - val_loss: 0.2718 - val_acc: 0.9247\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2814 - acc: 0.9215 - val_loss: 0.2658 - val_acc: 0.9275\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2744 - acc: 0.9231 - val_loss: 0.2594 - val_acc: 0.9279\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.2679 - acc: 0.9251 - val_loss: 0.2540 - val_acc: 0.9297\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 72us/step - loss: 0.2618 - acc: 0.9272 - val_loss: 0.2494 - val_acc: 0.9307\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.2560 - acc: 0.9285 - val_loss: 0.2441 - val_acc: 0.9316\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.2505 - acc: 0.9302 - val_loss: 0.2402 - val_acc: 0.9329\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2454 - acc: 0.9314 - val_loss: 0.2349 - val_acc: 0.9350\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.2405 - acc: 0.9331 - val_loss: 0.2312 - val_acc: 0.9354\n",
      "10000/10000 [==============================] - 1s 64us/step\n",
      "Test score: 0.2314021351069212\n",
      "Test accuracy: 0.9368\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=256\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=128,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5)  512 neurons,batch_size=128,epochs=20,Activation=relu and testScore: 0.9367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 128us/step - loss: 1.2249 - acc: 0.7230 - val_loss: 0.6758 - val_acc: 0.8590\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.5884 - acc: 0.8624 - val_loss: 0.4694 - val_acc: 0.8850\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.4627 - acc: 0.8823 - val_loss: 0.3983 - val_acc: 0.9003\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.4068 - acc: 0.8925 - val_loss: 0.3615 - val_acc: 0.9048\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.3738 - acc: 0.8996 - val_loss: 0.3376 - val_acc: 0.9103\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.3510 - acc: 0.9045 - val_loss: 0.3209 - val_acc: 0.9140\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.3334 - acc: 0.9083 - val_loss: 0.3077 - val_acc: 0.9167\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 7s 136us/step - loss: 0.3195 - acc: 0.9120 - val_loss: 0.2964 - val_acc: 0.9180\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.3074 - acc: 0.9150 - val_loss: 0.2864 - val_acc: 0.9209\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2972 - acc: 0.9173 - val_loss: 0.2785 - val_acc: 0.9226\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.2881 - acc: 0.9195 - val_loss: 0.2717 - val_acc: 0.9253\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.2800 - acc: 0.9224 - val_loss: 0.2646 - val_acc: 0.9265\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.2726 - acc: 0.9244 - val_loss: 0.2584 - val_acc: 0.9282\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.2656 - acc: 0.9262 - val_loss: 0.2531 - val_acc: 0.9300\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.2592 - acc: 0.9281 - val_loss: 0.2479 - val_acc: 0.9313\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.2531 - acc: 0.9296 - val_loss: 0.2424 - val_acc: 0.9327\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.2475 - acc: 0.9314 - val_loss: 0.2382 - val_acc: 0.9341\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.2422 - acc: 0.9327 - val_loss: 0.2335 - val_acc: 0.9354\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.2371 - acc: 0.9344 - val_loss: 0.2298 - val_acc: 0.9358\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.2322 - acc: 0.9356 - val_loss: 0.2254 - val_acc: 0.9373\n",
      "10000/10000 [==============================] - 1s 84us/step\n",
      "Test score: 0.22473093075454234\n",
      "Test accuracy: 0.9367\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=512\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=128,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6)  512 neurons,batch_size=512,epochs=20,Activation=relu and testScore: 0.9088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 1.8869 - acc: 0.5096 - val_loss: 1.4820 - val_acc: 0.7485\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 1.2520 - acc: 0.7736 - val_loss: 1.0245 - val_acc: 0.8217\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.9295 - acc: 0.8203 - val_loss: 0.7940 - val_acc: 0.8493\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.7600 - acc: 0.8425 - val_loss: 0.6664 - val_acc: 0.8652\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.6597 - acc: 0.8553 - val_loss: 0.5873 - val_acc: 0.8729\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.5939 - acc: 0.8649 - val_loss: 0.5335 - val_acc: 0.8802\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.5473 - acc: 0.8718 - val_loss: 0.4947 - val_acc: 0.8860\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.5125 - acc: 0.8765 - val_loss: 0.4654 - val_acc: 0.8894\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.4854 - acc: 0.8800 - val_loss: 0.4424 - val_acc: 0.8949\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.4635 - acc: 0.8833 - val_loss: 0.4240 - val_acc: 0.8963\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.4456 - acc: 0.8865 - val_loss: 0.4085 - val_acc: 0.8993\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.4304 - acc: 0.8891 - val_loss: 0.3957 - val_acc: 0.9012\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 3s 65us/step - loss: 0.4174 - acc: 0.8912 - val_loss: 0.3846 - val_acc: 0.9024\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.4062 - acc: 0.8928 - val_loss: 0.3751 - val_acc: 0.9037\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3962 - acc: 0.8952 - val_loss: 0.3666 - val_acc: 0.9047\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3874 - acc: 0.8966 - val_loss: 0.3590 - val_acc: 0.9061\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3795 - acc: 0.8983 - val_loss: 0.3521 - val_acc: 0.9076\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.3723 - acc: 0.8996 - val_loss: 0.3460 - val_acc: 0.9081\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 68us/step - loss: 0.3657 - acc: 0.9013 - val_loss: 0.3405 - val_acc: 0.9099\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.3597 - acc: 0.9029 - val_loss: 0.3355 - val_acc: 0.9112\n",
      "10000/10000 [==============================] - 1s 85us/step\n",
      "Test score: 0.33736450140476226\n",
      "Test accuracy: 0.9088\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=512\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=512,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)  512 neurons,batch_size=128,epochs=256,Activation=relu and testScore: 0.9247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 1.6285 - acc: 0.6111 - val_loss: 1.0616 - val_acc: 0.8171\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.8679 - acc: 0.8315 - val_loss: 0.6804 - val_acc: 0.8638\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.6372 - acc: 0.8593 - val_loss: 0.5408 - val_acc: 0.8796\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.5363 - acc: 0.8718 - val_loss: 0.4704 - val_acc: 0.8879\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.4793 - acc: 0.8806 - val_loss: 0.4278 - val_acc: 0.8933\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.4421 - acc: 0.8866 - val_loss: 0.3987 - val_acc: 0.8988\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.4154 - acc: 0.8911 - val_loss: 0.3777 - val_acc: 0.9024\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.3952 - acc: 0.8949 - val_loss: 0.3612 - val_acc: 0.9062\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.3789 - acc: 0.8979 - val_loss: 0.3482 - val_acc: 0.9082\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.3658 - acc: 0.9006 - val_loss: 0.3374 - val_acc: 0.9109\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.3544 - acc: 0.9032 - val_loss: 0.3281 - val_acc: 0.9119\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.3446 - acc: 0.9056 - val_loss: 0.3202 - val_acc: 0.9137\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.3361 - acc: 0.9078 - val_loss: 0.3133 - val_acc: 0.9140\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.3284 - acc: 0.9096 - val_loss: 0.3068 - val_acc: 0.9150\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.3214 - acc: 0.9114 - val_loss: 0.3010 - val_acc: 0.9176\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.3150 - acc: 0.9132 - val_loss: 0.2957 - val_acc: 0.9180\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 0.3091 - acc: 0.9145 - val_loss: 0.2914 - val_acc: 0.9188\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.3037 - acc: 0.9159 - val_loss: 0.2862 - val_acc: 0.9209\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.2987 - acc: 0.9173 - val_loss: 0.2820 - val_acc: 0.9219\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2938 - acc: 0.9185 - val_loss: 0.2780 - val_acc: 0.9229\n",
      "10000/10000 [==============================] - 1s 83us/step\n",
      "Test score: 0.2779099546611309\n",
      "Test accuracy: 0.9247\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=512\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=256,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8)  128 neurons,batch_size=64,epochs=20,Activation=relu and testScore: 0.9477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.9997 - acc: 0.7600 - val_loss: 0.5076 - val_acc: 0.8749\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.4625 - acc: 0.8785 - val_loss: 0.3812 - val_acc: 0.8978\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.3820 - acc: 0.8949 - val_loss: 0.3360 - val_acc: 0.9071\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.3441 - acc: 0.9038 - val_loss: 0.3111 - val_acc: 0.9124\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.3195 - acc: 0.9106 - val_loss: 0.2930 - val_acc: 0.9190\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.3010 - acc: 0.9157 - val_loss: 0.2788 - val_acc: 0.9211\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2861 - acc: 0.9202 - val_loss: 0.2660 - val_acc: 0.9244\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.2730 - acc: 0.9236 - val_loss: 0.2554 - val_acc: 0.9268\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.2614 - acc: 0.9271 - val_loss: 0.2465 - val_acc: 0.9301\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2508 - acc: 0.9304 - val_loss: 0.2376 - val_acc: 0.9333\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2413 - acc: 0.9329 - val_loss: 0.2299 - val_acc: 0.9351\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2325 - acc: 0.9352 - val_loss: 0.2225 - val_acc: 0.9381\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.2243 - acc: 0.9372 - val_loss: 0.2162 - val_acc: 0.9396\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2169 - acc: 0.9391 - val_loss: 0.2099 - val_acc: 0.9413\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.2096 - acc: 0.9412 - val_loss: 0.2051 - val_acc: 0.9437\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2033 - acc: 0.9432 - val_loss: 0.1996 - val_acc: 0.9443\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1971 - acc: 0.9447 - val_loss: 0.1949 - val_acc: 0.9455\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.1914 - acc: 0.9459 - val_loss: 0.1909 - val_acc: 0.9459\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1860 - acc: 0.9471 - val_loss: 0.1867 - val_acc: 0.9483\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1810 - acc: 0.9492 - val_loss: 0.1827 - val_acc: 0.9491\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "Test score: 0.18115492420345544\n",
      "Test accuracy: 0.9477\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=128\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=64,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9)  128 neurons,batch_size=32,epochs=20,Activation=relu and testScore: 0.9629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_56 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 7s 154us/step - loss: 0.7305 - acc: 0.8185 - val_loss: 0.3842 - val_acc: 0.8982\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.3648 - acc: 0.8998 - val_loss: 0.3125 - val_acc: 0.9140\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.3117 - acc: 0.9130 - val_loss: 0.2789 - val_acc: 0.9220\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.2804 - acc: 0.9208 - val_loss: 0.2568 - val_acc: 0.9278\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.2575 - acc: 0.9280 - val_loss: 0.2366 - val_acc: 0.9337\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.2386 - acc: 0.9336 - val_loss: 0.2227 - val_acc: 0.9373\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.2224 - acc: 0.9378 - val_loss: 0.2109 - val_acc: 0.9417\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 7s 141us/step - loss: 0.2087 - acc: 0.9416 - val_loss: 0.1998 - val_acc: 0.9452\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 7s 141us/step - loss: 0.1959 - acc: 0.9449 - val_loss: 0.1889 - val_acc: 0.9481\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.1850 - acc: 0.9479 - val_loss: 0.1798 - val_acc: 0.9517\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 7s 139us/step - loss: 0.1749 - acc: 0.9501 - val_loss: 0.1729 - val_acc: 0.9527\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.1660 - acc: 0.9535 - val_loss: 0.1659 - val_acc: 0.9553\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 7s 137us/step - loss: 0.1576 - acc: 0.9555 - val_loss: 0.1589 - val_acc: 0.9568\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.1504 - acc: 0.9577 - val_loss: 0.1534 - val_acc: 0.9592\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.1438 - acc: 0.9593 - val_loss: 0.1496 - val_acc: 0.9597\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 7s 151us/step - loss: 0.1376 - acc: 0.9617 - val_loss: 0.1446 - val_acc: 0.9617\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 7s 152us/step - loss: 0.1320 - acc: 0.9632 - val_loss: 0.1401 - val_acc: 0.9624\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.1267 - acc: 0.9650 - val_loss: 0.1371 - val_acc: 0.9632\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 7s 151us/step - loss: 0.1220 - acc: 0.9665 - val_loss: 0.1330 - val_acc: 0.9639\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 7s 149us/step - loss: 0.1177 - acc: 0.9673 - val_loss: 0.1292 - val_acc: 0.9650\n",
      "10000/10000 [==============================] - 1s 58us/step\n",
      "Test score: 0.12631398434378208\n",
      "Test accuracy: 0.9629\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=128\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10)  20 neurons,batch_size=32,epochs=20,Activation=relu and testScore: 0.9436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 20)                15700     \n",
      "_________________________________________________________________\n",
      "activation_59 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "activation_60 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 15,910\n",
      "Trainable params: 15,910\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 112us/step - loss: 0.9229 - acc: 0.7602 - val_loss: 0.4754 - val_acc: 0.8758\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.4299 - acc: 0.8812 - val_loss: 0.3510 - val_acc: 0.9015\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.3521 - acc: 0.9009 - val_loss: 0.3137 - val_acc: 0.9110\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.3172 - acc: 0.9099 - val_loss: 0.2880 - val_acc: 0.9170\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.2946 - acc: 0.9166 - val_loss: 0.2700 - val_acc: 0.9223\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.2773 - acc: 0.9214 - val_loss: 0.2578 - val_acc: 0.9258\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.2638 - acc: 0.9254 - val_loss: 0.2476 - val_acc: 0.9290\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.2524 - acc: 0.9280 - val_loss: 0.2392 - val_acc: 0.9312\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2425 - acc: 0.9306 - val_loss: 0.2326 - val_acc: 0.9333\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.2345 - acc: 0.9332 - val_loss: 0.2259 - val_acc: 0.9357\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.2274 - acc: 0.9348 - val_loss: 0.2216 - val_acc: 0.9361\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.2209 - acc: 0.9369 - val_loss: 0.2154 - val_acc: 0.9385\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.2152 - acc: 0.9384 - val_loss: 0.2123 - val_acc: 0.9387\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.2101 - acc: 0.9398 - val_loss: 0.2084 - val_acc: 0.9393\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2053 - acc: 0.9407 - val_loss: 0.2069 - val_acc: 0.9413\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.2013 - acc: 0.9427 - val_loss: 0.2016 - val_acc: 0.9421\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.1970 - acc: 0.9438 - val_loss: 0.1988 - val_acc: 0.9429\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.1931 - acc: 0.9448 - val_loss: 0.1957 - val_acc: 0.9444\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.1898 - acc: 0.9459 - val_loss: 0.1941 - val_acc: 0.9443\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.1864 - acc: 0.9461 - val_loss: 0.1912 - val_acc: 0.9456\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Test score: 0.1917158203408122\n",
      "Test accuracy: 0.9436\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=20\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11)  30 neurons,batch_size=32,epochs=20,Activation=relu and testScore: 0.9486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_62 (Dense)             (None, 30)                23550     \n",
      "_________________________________________________________________\n",
      "activation_61 (Activation)   (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 10)                310       \n",
      "_________________________________________________________________\n",
      "activation_62 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 23,860\n",
      "Trainable params: 23,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.7975 - acc: 0.7913 - val_loss: 0.4082 - val_acc: 0.8920\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.3901 - acc: 0.8920 - val_loss: 0.3282 - val_acc: 0.9084\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.3332 - acc: 0.9055 - val_loss: 0.2987 - val_acc: 0.9168\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.3036 - acc: 0.9135 - val_loss: 0.2777 - val_acc: 0.9225\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.2834 - acc: 0.9187 - val_loss: 0.2635 - val_acc: 0.9266\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.2676 - acc: 0.9236 - val_loss: 0.2510 - val_acc: 0.9302\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.2548 - acc: 0.9271 - val_loss: 0.2419 - val_acc: 0.9327\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.2437 - acc: 0.9313 - val_loss: 0.2359 - val_acc: 0.9347\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.2342 - acc: 0.9340 - val_loss: 0.2277 - val_acc: 0.9374\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.2257 - acc: 0.9359 - val_loss: 0.2209 - val_acc: 0.9378\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.2177 - acc: 0.9384 - val_loss: 0.2156 - val_acc: 0.9400\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.2105 - acc: 0.9403 - val_loss: 0.2127 - val_acc: 0.9406\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.2037 - acc: 0.9422 - val_loss: 0.2067 - val_acc: 0.9417\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.1975 - acc: 0.9439 - val_loss: 0.2006 - val_acc: 0.9449\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.1917 - acc: 0.9455 - val_loss: 0.1975 - val_acc: 0.9467\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.1865 - acc: 0.9473 - val_loss: 0.1942 - val_acc: 0.9453\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.1813 - acc: 0.9484 - val_loss: 0.1925 - val_acc: 0.9461\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.1765 - acc: 0.9498 - val_loss: 0.1866 - val_acc: 0.9489\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1722 - acc: 0.9511 - val_loss: 0.1842 - val_acc: 0.9494\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.1681 - acc: 0.9519 - val_loss: 0.1805 - val_acc: 0.9495\n",
      "10000/10000 [==============================] - 0s 46us/step\n",
      "Test score: 0.1753540755480528\n",
      "Test accuracy: 0.9486\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=30\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12)  60 neurons,batch_size=32,epochs=20,Activation=relu and testScore: 0.9569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             (None, 60)                47100     \n",
      "_________________________________________________________________\n",
      "activation_63 (Activation)   (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 10)                610       \n",
      "_________________________________________________________________\n",
      "activation_64 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 47,710\n",
      "Trainable params: 47,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 134us/step - loss: 0.7171 - acc: 0.8217 - val_loss: 0.3847 - val_acc: 0.8964\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.3664 - acc: 0.8976 - val_loss: 0.3148 - val_acc: 0.9113\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.3161 - acc: 0.9108 - val_loss: 0.2822 - val_acc: 0.9212\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.2876 - acc: 0.9177 - val_loss: 0.2614 - val_acc: 0.9271\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.2658 - acc: 0.9241 - val_loss: 0.2459 - val_acc: 0.9303\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2487 - acc: 0.9293 - val_loss: 0.2321 - val_acc: 0.9348\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.2341 - acc: 0.9333 - val_loss: 0.2216 - val_acc: 0.9380\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.2212 - acc: 0.9371 - val_loss: 0.2109 - val_acc: 0.9417\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 127us/step - loss: 0.2098 - acc: 0.9406 - val_loss: 0.2022 - val_acc: 0.9443\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.1996 - acc: 0.9435 - val_loss: 0.1936 - val_acc: 0.9469\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.1904 - acc: 0.9458 - val_loss: 0.1862 - val_acc: 0.9489\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.1823 - acc: 0.9481 - val_loss: 0.1806 - val_acc: 0.9499\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 125us/step - loss: 0.1749 - acc: 0.9504 - val_loss: 0.1764 - val_acc: 0.9512\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.1677 - acc: 0.9524 - val_loss: 0.1703 - val_acc: 0.9532\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.1617 - acc: 0.9537 - val_loss: 0.1653 - val_acc: 0.9543\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.1556 - acc: 0.9562 - val_loss: 0.1606 - val_acc: 0.9558\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.1502 - acc: 0.9577 - val_loss: 0.1565 - val_acc: 0.9570\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.1451 - acc: 0.9590 - val_loss: 0.1524 - val_acc: 0.9580\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.1405 - acc: 0.9607 - val_loss: 0.1493 - val_acc: 0.9587\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 113us/step - loss: 0.1360 - acc: 0.9615 - val_loss: 0.1459 - val_acc: 0.9604\n",
      "10000/10000 [==============================] - 0s 49us/step\n",
      "Test score: 0.1471345079049468\n",
      "Test accuracy: 0.9569\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=60\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13)  60 neurons,batch_size=32,epochs=20,Activation=relu and testScore: 0.9578"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 60)                47100     \n",
      "_________________________________________________________________\n",
      "activation_65 (Activation)   (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 10)                610       \n",
      "_________________________________________________________________\n",
      "activation_66 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 47,710\n",
      "Trainable params: 47,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.7516 - acc: 0.8111 - val_loss: 0.3945 - val_acc: 0.8923\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.3748 - acc: 0.8965 - val_loss: 0.3200 - val_acc: 0.9103\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.3216 - acc: 0.9095 - val_loss: 0.2863 - val_acc: 0.9198\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.2919 - acc: 0.9184 - val_loss: 0.2676 - val_acc: 0.9244\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 123us/step - loss: 0.2705 - acc: 0.9238 - val_loss: 0.2527 - val_acc: 0.9290\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 119us/step - loss: 0.2535 - acc: 0.9284 - val_loss: 0.2381 - val_acc: 0.9326\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.2392 - acc: 0.9324 - val_loss: 0.2280 - val_acc: 0.9369\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.2265 - acc: 0.9362 - val_loss: 0.2195 - val_acc: 0.9401\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.2151 - acc: 0.9392 - val_loss: 0.2110 - val_acc: 0.9423\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.2055 - acc: 0.9418 - val_loss: 0.2047 - val_acc: 0.9423\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.1965 - acc: 0.9446 - val_loss: 0.1973 - val_acc: 0.9462\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.1885 - acc: 0.9474 - val_loss: 0.1909 - val_acc: 0.9473\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.1812 - acc: 0.9493 - val_loss: 0.1853 - val_acc: 0.9484\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.1740 - acc: 0.9508 - val_loss: 0.1804 - val_acc: 0.9500\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 6s 117us/step - loss: 0.1675 - acc: 0.9524 - val_loss: 0.1759 - val_acc: 0.9507\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.1617 - acc: 0.9541 - val_loss: 0.1707 - val_acc: 0.9530\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.1560 - acc: 0.9561 - val_loss: 0.1661 - val_acc: 0.9546\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.1507 - acc: 0.9580 - val_loss: 0.1618 - val_acc: 0.9555\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.1459 - acc: 0.9590 - val_loss: 0.1582 - val_acc: 0.9566\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.1411 - acc: 0.9600 - val_loss: 0.1559 - val_acc: 0.9570\n",
      "10000/10000 [==============================] - 1s 51us/step\n",
      "Test score: 0.14624526520073414\n",
      "Test accuracy: 0.9578\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=60\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=32,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14)  32 neurons,batch_size=12,epochs=20,Activation=relu and testScore: 0.9641"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_68 (Dense)             (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "activation_67 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 10)                330       \n",
      "_________________________________________________________________\n",
      "activation_68 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 25,450\n",
      "Trainable params: 25,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 12s 250us/step - loss: 0.5496 - acc: 0.8490 - val_loss: 0.3080 - val_acc: 0.9138\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 11s 230us/step - loss: 0.2964 - acc: 0.9151 - val_loss: 0.2592 - val_acc: 0.9247\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 11s 233us/step - loss: 0.2538 - acc: 0.9269 - val_loss: 0.2336 - val_acc: 0.9349\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 11s 229us/step - loss: 0.2266 - acc: 0.9354 - val_loss: 0.2161 - val_acc: 0.9386\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 12s 246us/step - loss: 0.2057 - acc: 0.9414 - val_loss: 0.1967 - val_acc: 0.9452\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 12s 241us/step - loss: 0.1889 - acc: 0.9460 - val_loss: 0.1837 - val_acc: 0.9495\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.1761 - acc: 0.9490 - val_loss: 0.1755 - val_acc: 0.9510\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 12s 242us/step - loss: 0.1649 - acc: 0.9527 - val_loss: 0.1728 - val_acc: 0.9518\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 11s 234us/step - loss: 0.1552 - acc: 0.9554 - val_loss: 0.1618 - val_acc: 0.9531\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 12s 240us/step - loss: 0.1467 - acc: 0.9576 - val_loss: 0.1545 - val_acc: 0.9569\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 12s 240us/step - loss: 0.1396 - acc: 0.9599 - val_loss: 0.1506 - val_acc: 0.9578\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 11s 235us/step - loss: 0.1330 - acc: 0.9619 - val_loss: 0.1445 - val_acc: 0.9582\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 11s 233us/step - loss: 0.1270 - acc: 0.9636 - val_loss: 0.1508 - val_acc: 0.9566\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 11s 228us/step - loss: 0.1215 - acc: 0.9656 - val_loss: 0.1449 - val_acc: 0.9579\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 11s 233us/step - loss: 0.1167 - acc: 0.9670 - val_loss: 0.1396 - val_acc: 0.9601\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 11s 234us/step - loss: 0.1117 - acc: 0.9686 - val_loss: 0.1349 - val_acc: 0.9599\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 11s 231us/step - loss: 0.1084 - acc: 0.9698 - val_loss: 0.1314 - val_acc: 0.9614\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 11s 226us/step - loss: 0.1041 - acc: 0.9706 - val_loss: 0.1291 - val_acc: 0.9636\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 11s 238us/step - loss: 0.1002 - acc: 0.9716 - val_loss: 0.1277 - val_acc: 0.9634\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 11s 233us/step - loss: 0.0970 - acc: 0.9718 - val_loss: 0.1293 - val_acc: 0.9621\n",
      "10000/10000 [==============================] - 0s 46us/step\n",
      "Test score: 0.12015393814928829\n",
      "Test accuracy: 0.9641\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=32\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=12,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15)  10 neurons,batch_size=20,epochs=20,Activation=relu and testScore: 0.9269"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santhosh/anaconda3/envs/pydl/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 110us/step - loss: 0.7902 - acc: 0.7774 - val_loss: 0.4019 - val_acc: 0.8892\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.3931 - acc: 0.8880 - val_loss: 0.3358 - val_acc: 0.9037\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.3477 - acc: 0.9004 - val_loss: 0.3114 - val_acc: 0.9121\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.3258 - acc: 0.9075 - val_loss: 0.3007 - val_acc: 0.9157\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.3124 - acc: 0.9119 - val_loss: 0.2877 - val_acc: 0.9212\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.3025 - acc: 0.9136 - val_loss: 0.2894 - val_acc: 0.9201\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.2951 - acc: 0.9162 - val_loss: 0.2789 - val_acc: 0.9240\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.2885 - acc: 0.9175 - val_loss: 0.2758 - val_acc: 0.9236\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 0.2837 - acc: 0.9204 - val_loss: 0.2708 - val_acc: 0.9246\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.2792 - acc: 0.9212 - val_loss: 0.2710 - val_acc: 0.9256\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 5s 99us/step - loss: 0.2752 - acc: 0.9223 - val_loss: 0.2712 - val_acc: 0.9279\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.2722 - acc: 0.9233 - val_loss: 0.2650 - val_acc: 0.9279\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.2684 - acc: 0.9251 - val_loss: 0.2647 - val_acc: 0.9275\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.2659 - acc: 0.9253 - val_loss: 0.2615 - val_acc: 0.9301\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.2627 - acc: 0.9258 - val_loss: 0.2592 - val_acc: 0.9295\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.2596 - acc: 0.9275 - val_loss: 0.2559 - val_acc: 0.9297\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 107us/step - loss: 0.2572 - acc: 0.9280 - val_loss: 0.2620 - val_acc: 0.9286\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.2547 - acc: 0.9288 - val_loss: 0.2563 - val_acc: 0.9299\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.2524 - acc: 0.9302 - val_loss: 0.2504 - val_acc: 0.9319\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 113us/step - loss: 0.2495 - acc: 0.9305 - val_loss: 0.2517 - val_acc: 0.9317\n",
      "10000/10000 [==============================] - 0s 34us/step\n",
      "Test score: 0.25687655712664126\n",
      "Test accuracy: 0.9269\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=10\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=20,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16)  10 neurons,batch_size=10,epochs=20,Activation=relu and testScore: 0.9373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 9s 196us/step - loss: 0.6200 - acc: 0.8201 - val_loss: 0.3254 - val_acc: 0.9084\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 9s 193us/step - loss: 0.3208 - acc: 0.9072 - val_loss: 0.2949 - val_acc: 0.9184\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 9s 190us/step - loss: 0.2926 - acc: 0.9164 - val_loss: 0.2706 - val_acc: 0.9246\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 9s 186us/step - loss: 0.2767 - acc: 0.9205 - val_loss: 0.2632 - val_acc: 0.9260\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.2658 - acc: 0.9241 - val_loss: 0.2577 - val_acc: 0.9274\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.2565 - acc: 0.9269 - val_loss: 0.2478 - val_acc: 0.9304\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.2487 - acc: 0.9305 - val_loss: 0.2492 - val_acc: 0.9302\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 9s 183us/step - loss: 0.2409 - acc: 0.9318 - val_loss: 0.2440 - val_acc: 0.9340\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.2351 - acc: 0.9328 - val_loss: 0.2416 - val_acc: 0.9329\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 9s 184us/step - loss: 0.2302 - acc: 0.9353 - val_loss: 0.2331 - val_acc: 0.9347\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.2250 - acc: 0.9361 - val_loss: 0.2340 - val_acc: 0.9347\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.2206 - acc: 0.9372 - val_loss: 0.2295 - val_acc: 0.9365\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.2175 - acc: 0.9380 - val_loss: 0.2306 - val_acc: 0.9346\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.2139 - acc: 0.9392 - val_loss: 0.2240 - val_acc: 0.9375\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.2106 - acc: 0.9396 - val_loss: 0.2283 - val_acc: 0.9370\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.2079 - acc: 0.9404 - val_loss: 0.2229 - val_acc: 0.9375\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 9s 187us/step - loss: 0.2051 - acc: 0.9404 - val_loss: 0.2177 - val_acc: 0.9384\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 0.2018 - acc: 0.9420 - val_loss: 0.2183 - val_acc: 0.9397\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 0.1996 - acc: 0.9426 - val_loss: 0.2186 - val_acc: 0.9366\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 9s 184us/step - loss: 0.1976 - acc: 0.9432 - val_loss: 0.2161 - val_acc: 0.9398\n",
      "10000/10000 [==============================] - 0s 28us/step\n",
      "Test score: 0.22219503243938088\n",
      "Test accuracy: 0.9373\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=10\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=10,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17)  10 neurons,batch_size=5,epochs=20,Activation=relu and testScore: 0.9371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples shape (60000, 28, 28)\n",
      "10000 test samples (10000, 28, 28)\n",
      "60000 train samples shape (60000, 784)\n",
      "10000 test samples (10000, 784)\n",
      "before converting 7\n",
      "After converting [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,960\n",
      "Trainable params: 7,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 17s 358us/step - loss: 0.4828 - acc: 0.8591 - val_loss: 0.3107 - val_acc: 0.9075\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 17s 358us/step - loss: 0.3083 - acc: 0.9103 - val_loss: 0.2705 - val_acc: 0.9215\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 17s 364us/step - loss: 0.2800 - acc: 0.9190 - val_loss: 0.2697 - val_acc: 0.9212\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 17s 358us/step - loss: 0.2607 - acc: 0.9243 - val_loss: 0.2527 - val_acc: 0.9263\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 17s 361us/step - loss: 0.2473 - acc: 0.9273 - val_loss: 0.2411 - val_acc: 0.9283\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 17s 349us/step - loss: 0.2378 - acc: 0.9303 - val_loss: 0.2336 - val_acc: 0.9328\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 16s 343us/step - loss: 0.2300 - acc: 0.9319 - val_loss: 0.2349 - val_acc: 0.9313\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 17s 358us/step - loss: 0.2252 - acc: 0.9331 - val_loss: 0.2334 - val_acc: 0.9325\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 17s 345us/step - loss: 0.2202 - acc: 0.9351 - val_loss: 0.2275 - val_acc: 0.9363\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 16s 343us/step - loss: 0.2174 - acc: 0.9356 - val_loss: 0.2208 - val_acc: 0.9370\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 16s 341us/step - loss: 0.2132 - acc: 0.9367 - val_loss: 0.2181 - val_acc: 0.9377\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 17s 344us/step - loss: 0.2100 - acc: 0.9375 - val_loss: 0.2258 - val_acc: 0.9348\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 17s 351us/step - loss: 0.2081 - acc: 0.9383 - val_loss: 0.2471 - val_acc: 0.9271\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 17s 351us/step - loss: 0.2051 - acc: 0.9388 - val_loss: 0.2318 - val_acc: 0.9316\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 16s 339us/step - loss: 0.2024 - acc: 0.9391 - val_loss: 0.2417 - val_acc: 0.9287\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 17s 347us/step - loss: 0.2014 - acc: 0.9395 - val_loss: 0.2310 - val_acc: 0.9323\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 17s 346us/step - loss: 0.1996 - acc: 0.9400 - val_loss: 0.2279 - val_acc: 0.9329\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 17s 348us/step - loss: 0.1969 - acc: 0.9410 - val_loss: 0.2351 - val_acc: 0.9295\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 17s 345us/step - loss: 0.1964 - acc: 0.9404 - val_loss: 0.2362 - val_acc: 0.9322\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 16s 344us/step - loss: 0.1951 - acc: 0.9418 - val_loss: 0.2209 - val_acc: 0.9343\n",
      "10000/10000 [==============================] - 0s 27us/step\n",
      "Test score: 0.2118680354088545\n",
      "Test accuracy: 0.9371\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#reshaping the x_test,x_train to 60000*784\n",
    "reshape=784\n",
    "x_train=x_train.reshape(60000,784)\n",
    "x_test=x_test.reshape(10000,784)\n",
    "print(x_train.shape[0], 'train samples','shape',x_train.shape)\n",
    "print(x_test.shape[0], 'test samples',x_test.shape)\n",
    "\n",
    "#change int to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#normalize the traing and testing data\n",
    "x_train=x_train/255\n",
    "x_test=x_test/255\n",
    "\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "from keras.utils import np_utils\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "print('before converting',y_test[0])\n",
    "y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "print( 'After converting',y_test[0])\n",
    "\n",
    "#we now create a model(with 2 hidden layers of each 128 neurons) with 10 neurans(since we use 10 class labels),and input is 784(features)\n",
    "#our activation function is softmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense,Activation\n",
    "N_HIDDEN=10\n",
    "model=Sequential()\n",
    "model.add(Dense(N_HIDDEN,input_shape=(reshape,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "#now we need compile the model,with optimser as SGD,loss function as  Categorical cross-entropy and to evaluate we use accuracy metrics\n",
    "from keras.optimizers import SGD\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#now we train our model with epochs=200, and batch size as 128 and validation_split=0.2,verbose=1(progress bar)\n",
    "history=model.fit(x_train,y_train,epochs=20,batch_size=5,validation_split=0.2,verbose=1)\n",
    "\n",
    "#finally we evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
